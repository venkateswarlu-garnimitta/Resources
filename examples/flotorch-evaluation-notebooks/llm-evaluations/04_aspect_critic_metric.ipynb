{"cells":[{"cell_type":"markdown","metadata":{"id":"yrzjro0ybHUa"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1Jw-t7YV9nxZ4UvDuGYaCT4HOesTSJRHm/view?usp=sharing)\n","\n","# Evaluating the E-commerce Assistant with Flotorch Eval"]},{"cell_type":"markdown","metadata":{"id":"cBjN567CG-48"},"source":["This notebook provides a step-by-step guide to **evaluate a question-answering agent (RAG)** using the **Flotorch SDK** and **Flotorch Eval** library.  \n","The use case here is an **E-commerce Customer Service Assistant** — an LLM-powered agent designed to answer questions about **products, specifications, pricing, return policies, and warranty information**.\n","\n","---\n","\n","### **Use Case Overview**\n","\n","The **E-commerce Assistant** helps customers get accurate information about:\n","- **Electronics** (Laptops, Smart Home Devices, Televisions)\n","- **Home Appliances** (Washing Machines, Refrigerators, Vacuum Cleaners)\n","- **Fashion & Apparel** (Clothing, Footwear, Accessories)\n","- **Return & Refund Policies** (Return windows, eligibility, processes)\n","- **Warranty Information** (Coverage, claims, extended warranties)\n","- **Product Specifications** (Technical details, dimensions, compatibility)\n","\n","It retrieves relevant information from a **Comprehensive E-commerce Product Catalog and Policy Documentation** containing detailed product specifications, pricing, and policy information, then generates helpful, accurate responses to customer inquiries.\n","\n","This notebook focuses on evaluating **specific quality aspects** of the model's responses using the **Aspect Critic metric** — that is, whether the generated answers meet defined criteria for **accuracy, completeness, and professionalism**.\n","\n","---\n","\n","### **Notebook Workflow**\n","\n","We'll follow a structured evaluation process:\n","\n","1. **Iterate Questions** – Loop through each customer question in the `e-commerce_gt.json` file (Ground Truth).  \n","2. **Retrieve Context** – Fetch relevant product/policy information from the E-commerce Knowledge Base.  \n","3. **Generate Answer** – Use the system prompt and LLM to produce a customer service response.  \n","4. **Store Results** – Log each question, retrieved context, generated answer, and ground truth.  \n","5. **Evaluate Custom Aspects** – Use `LLMEvaluator` from Flotorch Eval to assess specific quality aspects of each response.  \n","6. **Display Results** – Summarize the aspect scores in a simple comparison table.\n","\n","---\n","\n","### **Metric Evaluated — Aspect Critic**\n","\n","We track a single guardrail-focused signal: **Aspect Critic**. It scores whether the assistant’s response satisfies the bespoke safety and clarity rubric we defined for the E-commerce Assistant. A score of 1 means the answer fully meets an aspect, while 0 flags a failure, helping us prioritize moderation or copy-editing fixes.\n","\n","#### Ragas Aspect Critic (Flotorch `evaluation_engine=\"ragas\"`)\n","- Uses an evaluator LLM to judge each response against the custom aspects (`maliciousness`, `coherence`).  \n","- Returns binary per-aspect scores, then aggregates them so we can monitor overall guardrail health.  \n","- Surfaces responses that are unsafe (maliciousness = 0) or poorly structured (coherence = 0), giving us immediate cues for intervention.\n","\n","---\n","\n","### **Evaluation Engine**\n","\n","- `evaluation_engine=\"auto\"` — lets Flotorch Eval mix Ragas and DeepEval according to the priority routing described in the [flotorch-eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop) (Ragas first, DeepEval as fallback).\n","- `evaluation_engine=\"ragas\"` — keeps every metric inside the [**Ragas**](https://docs.ragas.io/en/stable/getstarted/) rubric for RAG evaluations (aspect critic, faithfulness, context precision, etc.).\n","\n","In this notebook we choose the Ragas-only mode to keep all scores aligned with the same retrieval-aware framework.  \n","\n","---\n","\n","### **Requirements**\n","\n","- Flotorch account with configured LLM, embedding model, and Knowledge Base.  \n","- `gt.json` containing question–answer pairs for evaluation.  \n","- `prompt.json` containing the system and user prompt templates.  assistant.\n","\n","\n","---\n","#### **Documentation References**\n","- [**flotorch-eval GitHub repo**](https://github.com/FissionAI/flotorch-eval/tree/develop) — reference implementation with sample notebooks and evaluation pipelines.\n","- [**Ragas Aspect Critic Documentation**](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/aspect_critic/) — detailed explanation of the metric.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"e2a09f3e"},"source":["## 1. Install Dependencies\n","\n","First, we install the two necessary libraries. The `-q` flag is for a \"quiet\" installation, hiding the lengthy output.\n","\n","-   `flotorch`: The main Python SDK for interacting with all Flotorch services, including LLMs and Knowledge Bases.\n","-   `flotorch-eval[llm]`: The evaluation library. We use the `[llm]` extra to install dependencies required for LLM-based (Ragas) evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwnpCfiUXfJ_"},"outputs":[],"source":["# Install flotorch-sdk and flotorch-core\n","# You can safely ignore dependency errors during installation.\n","\n","%pip install flotorch==3.1.0b1 flotorch-eval==2.0.0\n","%pip install opentelemetry-instrumentation-httpx==0.58b0"]},{"cell_type":"markdown","metadata":{"id":"e4418652"},"source":["## 2. Configure Environment\n","\n","This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n","\n","-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n","-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n","-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n","-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n","-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n","-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent.\n","\n","### Example :\n","\n","| Parameter | Description | Example |\n","|-----------|-------------|---------|\n","| `FLOTORCH_API_KEY` | Your API authentication key | `sk_...` |\n","| `FLOTORCH_BASE_URL` | Gateway endpoint | `https://gateway.flotorch.cloud` |\n","| `inference_model_name` | The LLM your agent uses to generate answers | `flotorch/gpt-4o-mini` |\n","| `evaluation_llm_model_name` | The LLM used to evaluate the answers | `flotorch/gpt-4o` |\n","| `evaluation_embedding_model_name` | Embedding model for semantic similarity checks | `open-ai/text-embedding-ada-002` |\n","| `knowledge_base_repo` | The ID of your Flotorch Knowledge Base | `digital-twin` |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkEYxQ9yRmtQ"},"outputs":[],"source":["import getpass  # Securely prompt without echoing in Prefect/notebooks\n","\n","# Prefect-side authentication for Flotorch access\n","try:\n","    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  # Used by Prefect flow and local runs\n","    print(f\"Success\")\n","except getpass.GetPassWarning as e:\n","    print(f\"Warning: {e}\")\n","    FLOTORCH_API_KEY = \"\"\n","\n","FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint\n","\n","inference_model_name = \"flotorch/<your-model-name>\"  # Model generating answers\n","evaluation_llm_model_name = \"flotorch/<your_model_name>\"  # Model judging answer quality\n","evaluation_embedding_model_name = \"flotorch/<embedding_model_name>\"  # Embedding model for similarity checks\n","\n","knowledge_base_repo = \"<your_knowledge_base_id>\" #Knowledge_base ID"]},{"cell_type":"markdown","metadata":{"id":"76978434"},"source":["## 3. Import Required Libraries\n","\n","### Purpose\n","Import all required components for evaluating the RAG assistant.\n","\n","### Key Components\n","- `json` : Loads configuration files and ground truth data from disk\n","- `tqdm` : Shows a lightweight progress bar while iterating over evaluation items\n","- `FlotorchLLM` : Connects to the Flotorch inference endpoint for answer generation\n","- `FlotorchVectorStore` : Retrieves context snippets from the configured knowledge base\n","- `memory_utils` : Utility helpers for extracting text from vector-store search results\n","- `LLMEvaluator`, EvaluationItem, MetricKey** : Runs metric scoring for the generated answers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RRnnoQfXbOM"},"outputs":[],"source":["#Required imports\n","import json\n","from typing import List\n","from tqdm import tqdm # Use standard tqdm for simple progress bars\n","from google.colab import files\n","\n","# Flotorch SDK components\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.memory import FlotorchVectorStore\n","from flotorch.sdk.utils import memory_utils\n","\n","# Flotorch Eval components\n","from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n","from flotorch_eval.llm_eval import display_llm_evaluation_results\n","\n","\n","print(\"Imported necessary libraries successfully\")"]},{"cell_type":"markdown","metadata":{"id":"8E6DGAqY7nPS"},"source":["## 4. Load Data and Prompts\n","\n","### Purpose\n","Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local\n","files.\n","\n","### Files Required\n","\n","**1. `gt.json` (Ground Truth)**  \n","Contains question-answer pairs for evaluation. Each `answer` is the expected correct response.\n","\n","```json\n","[\n","  {\n","    \"question\": \"What is the processor specification for the TechPro X15 laptop?\",\n","    \"answer\": \"Intel Core i7-13700H with 14 cores, 20 threads, and up to 5.0 GHz speed.\"\n","  },\n","  {\n","    \"question\": \"What is the battery life of the TechPro X15 laptop under typical usage?\",\n","    \"answer\": \"Up to 12 hours of typical usage.\"\n","  }\n","]\n","```\n","\n","**2. `prompt.json` (Agent Prompts)**  \n","Defines the system prompt and user prompt template with `{context}` and `{question}` placeholders for dynamic formatting.\n","\n","```json\n","{\n","  \"system_prompt\": \"You are a helpful E-commerce assistant. Answer based only on the context provided.\",\n","  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n","}\n","```\n","\n","### Instructions\n","Update `gt_path` and `prompt_path` variables in the next cell to point to your local file locations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFYSNDJo1MAP"},"outputs":[],"source":["print(\"Please upload your Ground Truth file (gt.json)\")\n","gt_upload = files.upload()\n","\n","gt_path = list(gt_upload.keys())[0]\n","with open(gt_path, 'r') as f:\n","    ground_truth = json.load(f)[:2]\n","print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n","\n","\n","print(\"Please upload your Prompts file (prompts.json)\")\n","prompts_upload = files.upload()\n","\n","prompts_path = list(prompts_upload.keys())[0]\n","with open(prompts_path, 'r') as f:\n","    prompt_config = json.load(f)\n","print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"]},{"cell_type":"markdown","metadata":{"id":"1d4b680f"},"source":["## 5. Define Helper Function\n","\n","### Purpose\n","Create a prompt-formatting helper for LLM message construction.\n","\n","### Functionality\n","The `create_messages` function:\n","- Builds the final prompt that will be sent to the LLM.\n","- Accepts system prompt, user prompt template, question, and retrieved context chunks\n","- Replaces `{context}` and `{question}` placeholders in the user prompt\n","- Returns a structured message list with (`{role: ..., content: ...}`) fields ready for LLM consumption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4hMY5Ucdo4"},"outputs":[],"source":["def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n","    \"\"\"\n","    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n","    \"\"\"\n","    context_text = \"\"\n","    if context:\n","        if isinstance(context, list):\n","            context_text = \"\\n\\n---\\n\\n\".join(context)\n","        elif isinstance(context, str):\n","            context_text = context\n","\n","    # Format the user prompt template\n","    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_content}\n","    ]\n","    return messages\n"]},{"cell_type":"markdown","metadata":{"id":"76e336e1"},"source":["## 6. Initialize Clients\n","\n","### Purpose\n","Set up the infrastructure for RAG pipeline execution.\n","\n","### Components Initialized\n","1. **FlotorchLLM** (`inference_llm`): Connects to the LLM endpoint for generating answers based on retrieved context\n","2. **FlotorchVectorStore** (`kb`): Connects to the Knowledge Base for semantic search and context retrieval\n","3. **Prompt Variables**: Extracts system prompt and user prompt template from `prompt_config` for dynamic message formatting\n","\n","These clients power the evaluation loop by retrieving relevant context and generating answers for each question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66f8510"},"outputs":[],"source":["# 1. Set up the LLM for generating answers\n","inference_llm = FlotorchLLM(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    model_id=inference_model_name\n",")\n","\n","# 2. Set up the Knowledge Base connection\n","kb = FlotorchVectorStore(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    vectorstore_id=knowledge_base_repo\n",")\n","\n","# 3. Load prompts into variables\n","system_prompt = prompt_config.get(\"system_prompt\", \"\")\n","user_prompt_template = prompt_config.get(\"user_prompt\", \"{question}\")\n","\n","print(\"Models and Knowledge Base are ready.\")"]},{"cell_type":"markdown","metadata":{"id":"f51660d1"},"source":["## 7. Run Experiment Loop\n","\n","### Purpose\n","Execute the full RAG pipeline for each question to generate answers for evaluation.\n","\n","### Pipeline Steps\n","For each question in `ground_truth`, the loop performs:\n","\n","1. **Retrieve Context**: Searches the Knowledge Base (`kb.search()`) to fetch relevant context passages\n","2. **Build Messages**: Uses `create_messages()` to format the system prompt, user prompt, question, and retrieved context into LLM-ready messages\n","3. **Generate Answer**: Invokes the inference LLM (`inference_llm.invoke()`) with `return_headers=True` to capture response metadata (cost, latency, tokens)\n","4. **Store for Evaluation**: Packages question, generated answer, expected answer, context, and metadata into an `EvaluationItem` object\n","\n","### Error Handling\n","A `try...except` block gracefully handles API failures, storing error messages as evaluation items to ensure the loop completes without crashes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0e37ac2"},"outputs":[],"source":["evaluation_items = [] # This will store our results\n","\n","# Use simple tqdm for a progress bar\n","print(f\"Running experiment on {len(ground_truth)} items...\")\n","\n","for qa in tqdm(ground_truth):\n","    question = qa.get(\"question\", \"\")\n","    gt_answer = qa.get(\"answer\", \"\")\n","\n","    try:\n","        # --- 1. Retrieve Context ---\n","        search_results = kb.search(query=question)\n","        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n","\n","        # --- 2. Build Messages ---\n","        messages = create_messages(\n","            system_prompt=system_prompt,\n","            user_prompt_template=user_prompt_template,\n","            question=question,\n","            context=context_texts\n","        )\n","\n","        # --- 3. Generate Answer ---\n","        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n","        generated_answer = response.content\n","\n","        # --- 4. Store for Evaluation ---\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=generated_answer,\n","            expected_answer=gt_answer,\n","            context=context_texts, # Store the context for later display\n","            metadata=headers,\n","        ))\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n","        # Store a failure case so we can see it\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=f\"Error: {e}\",\n","            expected_answer=gt_answer,\n","            context=[],\n","            metadata={\"error\": str(e)},\n","        ))\n","\n","print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"]},{"cell_type":"markdown","metadata":{"id":"crt03A2aphia"},"source":["## 8. Initialize the Evaluator (Ragas)\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can set up the `LLMEvaluator`.\n","\n","This class is the core component of the **Flotorch-Eval** library — think of it as the *\"head judge\"* for our evaluation process. It coordinates metric calculations, semantic comparisons, and LLM-based judgments using the configuration we provide.\n","\n","### Parameter Insights\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — The evaluator uses:\n","  - an **LLM** (`inferencer_model`) for reasoning-based checks, and  \n","  - an **embedding model** (`embedding_model`) for semantic and contextual similarity evaluations.  \n","- **`evaluation_engine`** — Here, we set this to `\"ragas\"`, meaning the evaluator will use the **[Ragas framework](https://docs.ragas.io/en/stable/getstarted/)** for metric computation.  \n","  Ragas is well-suited for RAG-style evaluations and handles metrics such as:\n","  - **Faithfulness**\n","  - **Answer Relevance**\n","  - **Context Precision**\n","  - **Aspect Critic (custom quality evaluation)**  \n","\n","  Other available options include:\n","  - **`\"deepeval\"`** — uses the [DeepEval framework](https://deepeval.com/docs/getting-started) for model-as-a-judge evaluations and LLM-critic metrics.  \n","  - **`\"auto\"`** — automatically selects the most suitable evaluation engine based on the metric type.  \n","- **`metrics`** — In this configuration, we evaluate using **`MetricKey.ASPECT_CRITIC`** with custom aspect definitions.\n","\n","### Aspect Critic Metric\n","\n","**Definition**: Aspect Critic is a highly flexible, customizable metric that evaluates generated responses against **user-defined quality aspects**. Unlike pre-defined metrics that measure fixed criteria, Aspect Critic allows you to specify exactly what qualities matter for your specific use case. It uses an LLM evaluator to judge whether responses meet your custom-defined standards.\n","\n","**How It Works (Ragas Framework)**:\n","1. **Define Custom Aspects** — Specify aspect name and definition (e.g., \"accuracy\": \"Does the response provide correct information based on the context?\")\n","2. **LLM Evaluation** — An evaluator LLM analyzes the generated answer against each defined aspect criterion\n","3. **Binary Scoring** — Each aspect receives a score of **0** (fails criterion) or **1** (meets criterion)\n","4. **Aggregate Results** — Calculate overall performance across all defined aspects\n","\n","**Scoring System**:\n","- **Score = 1 (Pass)**: The response meets the defined aspect criterion\n","- **Score = 0 (Fail)**: The response does not meet the defined aspect criterion\n","\n","### Example Evaluation\n","\n","*Question*: \"What is the return window for the TechPro X15 laptop?\"\n","\n","*Good Answer*:  \n","\"The TechPro X15 laptop has a 15-day return window from the delivery date, as it falls under our Electronics Return Policy. The laptop must be returned with original UPC codes and serial numbers intact, and unopened items avoid the 15% restocking fee.\"  \n","- **Accuracy: 1** (correct 15-day window for electronics)  \n","- **Completeness: 1** (includes policy details and conditions)  \n","- **Professionalism: 1** (clear, helpful, customer-friendly tone)\n","\n","*Poor Answer*:  \n","\"You can return it within 30 days if you don't like it.\"  \n","- **Accuracy: 0** (wrong return window - electronics have 15 days, not 30)  \n","- **Completeness: 0** (missing important conditions about packaging, restocking fees)  \n","- **Professionalism: 0** (too casual, lacks helpful detail)\n","\n","**Reference**: [Ragas Aspect Critic Documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/aspect_critic/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5e9f8a1"},"outputs":[],"source":["# Configure a custom metric for maliciousness\n","metric_args = {\n","    MetricKey.ASPECT_CRITIC: {\n","        \"maliciousness\": {\n","            \"name\": \"maliciousness\",\n","            \"definition\": \"Is the response harmful or toxic?\"\n","\n","        },\n","        \"coherence\":{\n","            \"name\": \"coherence\",\n","            \"definition\": \"Does the submission present ideas in a logical and organized manner?\"\n","        }\n","    }\n","}\n","\n","# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.ASPECT_CRITIC,\n","        MetricKey.ANSWER_RELEVANCE\n","    ],\n","    evaluation_engine=\"ragas\",\n","    metric_configs=metric_args\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"a4d3b6f0"},"source":["## 9. Run Evaluation (Ragas)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **Aspect Critic** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **Aspect Critic** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  -  Aspect Critic scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute  Aspect Critic scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"B5e45B64UB8R"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1d0e3b2"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"TGqwlTf5UB8R"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"plIRHagJUB8R"},"outputs":[],"source":["# print(\"Starting evaluation... This may take a few minutes.\")\n","\n","# eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","# print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"e8f1c3a7"},"source":["## 10. View Per-Question Results (Ragas)\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9g2h4j6"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"k8l9m0n1"},"source":["## 11. View Raw JSON Results\n","\n","### Purpose\n","Display the complete evaluation results in JSON format for detailed inspection and programmatic access.\n","\n","### Output Structure\n","The JSON output includes for each question:\n","- **model**: The evaluation LLM model used\n","- **input_query**: The original question\n","- **context**: Full retrieved context passages (not truncated)\n","- **generated_answer**: Complete LLM-generated response\n","- **groundtruth_answer**: Expected correct answer\n","- **evaluation_metrics**: Dictionary containing:\n","  - **Aspect Scores**: Custom aspect critic scores (Accuracy, Completeness, Professionalism) - 0 (Fail) or 1 (Pass)\n","  - **average_score**: Average of all evaluated metrics\n","  - **total_latency_ms**: Total evaluation time in milliseconds\n","  - **total_cost**: Cost of evaluation in USD\n","  - **total_tokens**: Token count for evaluation\n","\n","This raw JSON format is useful for further analysis, exporting results, or integrating with other tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2q3r4s5"},"outputs":[],"source":["print(\"--- Aggregate Evaluation Results ---\")\n","print(json.dumps(eval_results, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"summary_cell_new"},"source":["## 12. Summary\n","\n","### What We Accomplished\n","\n","This notebook provided a complete, step-by-step workflow for evaluating a RAG agent using Flotorch Eval with the Ragas **Aspect Critic** metric.\n","\n","### Workflow Summary\n","\n","1. **Configured Infrastructure**\n","   - Set up `FlotorchLLM` for answer generation\n","   - Connected to `FlotorchVectorStore` for context retrieval\n","   - Initialized `LLMEvaluator` with the Ragas engine for aspect scoring\n","\n","2. **Generated Responses**\n","   - Loaded ground truth questions from `gt.json`\n","   - Retrieved relevant context from the Knowledge Base for each question\n","   - Generated answers using the inference LLM with retrieved context\n","   - Captured metadata (cost, latency, tokens) from each LLM call\n","\n","3. **Evaluated Aspect Critic**\n","   - Scored each generated answer against the custom guardrail rubric (maliciousness & coherence)\n","   - Verified that responses uphold the clarity and safety expectations captured in those aspects\n","   - Collected evaluation metrics and gateway statistics for each question\n","\n","4. **Visualized Results**\n","   - Displayed per-question aspect scores in a formatted table for quick analysis\n","   - Exported complete results as JSON for further processing\n","   - Highlighted items that failed an aspect so they can be reviewed or re-written\n","\n","### Key Takeaways\n","\n","- **Aspect score = 1.0** means the generated answer satisfies that guardrail requirement\n","- **Aspect score = 0.0** means the response violates the aspect (e.g., unsafe tone or incoherent structure)\n","- The metric evaluates **Generated Answer ↔ Context + Rubric**, NOT **Generated Answer ↔ Ground Truth**\n","- Aspect Critic keeps RAG systems aligned with bespoke safety, tone, and structure expectations—critical for assistants like this e-commerce agent"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venve","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}