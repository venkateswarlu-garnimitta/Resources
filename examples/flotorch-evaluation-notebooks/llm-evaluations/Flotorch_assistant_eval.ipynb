{"cells":[{"cell_type":"markdown","metadata":{"id":"XvCX9BgW3BmD"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1eNJlbx1PBrVeC2A3NsUrDZehRF2mvcmu/view?usp=sharing)\n","\n","# Evaluating the Flotorch Product Assistant with Flotorch Eval"]},{"cell_type":"markdown","metadata":{"id":"POKsV852T0Rd"},"source":["\n","\n","In this notebook, we perform a structured evaluation of a **RAG-based QA system** using the **Flotorch SDK** together with the **Flotorch Eval** framework.\n","\n","---\n","\n","### **Use Case Overview**\n","\n","The **Flotorch Product Assistant** helps users understand and navigate Flotorch’s tools, SDKs, and workflows.  \n","It retrieves information from a knowledge base populated with **Flotorch product documentation** and generates accurate, helpful answers to user queries.\n","\n","This notebook evaluates how well the assistant performs in providing **relevant, factual, and safe** responses.\n","\n","---\n","\n","### **Notebook Workflow**\n","\n","We’ll go through the following steps:\n","\n","1. **Iterate Questions** – Loop through each question in a `gt.json` (Ground Truth) file.  \n","2. **Retrieve Context** – Query the Flotorch Knowledge Base for relevant information.  \n","3. **Generate Answer** – Use the system prompt and Flotorch LLM to produce an answer.  \n","4. **Store Results** – Save each question, retrieved context, generated answer, and ground truth.  \n","5. **Evaluate** – Use `LLMEvaluator` from Flotorch Eval to assess the outputs.  \n","6. **Display Scores** – Show the evaluation results in a simple table.\n","\n","---\n","\n","### **Metrics Evaluated**\n","\n","To measure the assistant’s quality, we’ll use four key metrics:\n","\n","- **Faithfulness** – Is the answer supported by the retrieved context?  \n","- **Answer Relevancy** – Does it directly address the user’s question?  \n","- **Context Precision** – Is the retrieved context appropriate and focused?  \n","- **Hallucination** – This metric assesses whether the model generates factually incorrect or fabricated information that is not supported by the input or context.\n","\n","---\n","\n","### **Requirements**\n","\n","- Flotorch account with configured LLM, embedding model, and Knowledge Base.  \n","- `gt.json` containing question–answer pairs for evaluation.  \n","- `prompt.json` containing the system and user prompt templates.\n"]},{"cell_type":"markdown","metadata":{"id":"e2a09f3e"},"source":["## 1. Install Dependencies\n","\n","Before proceeding, install the required libraries for model interaction and evaluation:\n","\n","- **`flotorch`** — The primary Python SDK for accessing Flotorch services, including LLM inference, knowledge bases, and related APIs.  \n","- **`flotorch-eval[all]`** — The evaluation toolkit. The `[all]` option ensures that all optional dependencies needed for metrics, evaluation engines, and integrations are installed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"uwnpCfiUXfJ_"},"outputs":[],"source":["# Install flotorch-sdk and flotorch-core\n","# You can safely ignore dependency errors during installation.\n","\n","%pip install flotorch==3.1.0b1 flotorch-eval==2.0.0\n","%pip install opentelemetry-instrumentation-httpx==0.58b0"]},{"cell_type":"markdown","metadata":{"id":"e4418652"},"source":["## 2. Configure Environment\n","\n","This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n","\n","-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n","-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n","-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n","-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n","-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n","-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmsSeVgpWceZ"},"outputs":[],"source":["# -----------------------------------------------------------\n","# Flotorch Model Configuration\n","# -----------------------------------------------------------\n","# Update the placeholders below with the correct values\n","# from your Flotorch Console before running inference\n","# or evaluation.\n","# -----------------------------------------------------------\n","\n","from getpass import getpass\n","\n","# Authentication\n","FLOTORCH_API_KEY = getpass(\"Enter your Flotorch API key: \")\n","FLOTORCH_BASE_URL = input(\"Enter Base URL: \")   # Example: \"https://gateway.flotorch.cloud\"\n","\n","# Model names (replace with your actual model IDs)\n","inference_model_name = \"<INFERENCE_MODEL_NAME>\"                # Model used by your agent\n","evaluation_llm_model_name = \"<EVALUATION_LLM_MODEL_NAME>\"      # Model used for scoring\n","evaluation_embedding_model_name = \"<EMBEDDING_MODEL_NAME>\"     # Embedding model for Eval,\n","                                                               # Example:\"flotorch/text-embedding-model\n","\n","# Knowledge base repository used for retrieval\n","knowledge_base_repo = \"<KNOWLEDGE_BASE_REPO_NAME>\"\n"]},{"cell_type":"markdown","metadata":{"id":"76978434"},"source":["## 3. Import Libraries\n","\n","Next, we import all the necessary modules from Python and the Flotorch libraries that we'll use throughout the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RRnnoQfXbOM"},"outputs":[],"source":["#Required imports\n","import json\n","from typing import List\n","from tqdm import tqdm # Use standard tqdm for simple progress bars\n","\n","# Flotorch SDK components\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.memory import FlotorchVectorStore\n","from flotorch.sdk.utils import memory_utils\n","from google.colab import files\n","\n","# Flotorch Eval components\n","from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n","from flotorch_eval import display_llm_evaluation_results"]},{"cell_type":"markdown","metadata":{"id":"8E6DGAqY7nPS"},"source":["## 4. Load Data and Prompts\n","\n","Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local files.\n","\n","Your files should be structured as follows:\n","\n","**`gt.json` (Ground Truth)**\n","A list of question-answer objects. The `answer` is the \"perfect\" response you are testing against.\n","```json\n","[\n","  {\n","    \"question\": \"What is Flotorch?\",\n","    \"answer\": \"Flotorch is an end-to-end platform for...\"\n","  },\n","  {\n","    \"question\": \"How do I install the SDK?\",\n","    \"answer\": \"You can install it using pip: pip install flotorch.\"\n","  }\n","]\n","```\n","\n","**`prompt.json` (Agent Prompts)**\n","Contains the system prompt and the user prompt template. Note the `{context}` and `{question}` placeholders, which we will fill dynamically.\n","```json\n","{\n","  \"system_prompt\": \"You are a helpful Flotorch product assistant. Answer based only on the context provided.\",\n","  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n","}\n","```\n","\n","**Note:** In Google Colab, you can use the file icon on the left to upload your files and then adjust the paths in `gt_path` and `prompt_path`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFYSNDJo1MAP"},"outputs":[],"source":["print(\"Please upload your Ground Truth file (gt.json)\")\n","gt_upload = files.upload()\n","\n","gt_path = list(gt_upload.keys())[0]\n","with open(gt_path, 'r') as f:\n","    ground_truth = json.load(f)\n","print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n","\n","\n","print(\"Please upload your Prompts file (prompts.json)\")\n","prompts_upload = files.upload()\n","\n","prompts_path = list(prompts_upload.keys())[0]\n","with open(prompts_path, 'r') as f:\n","    prompt_config = json.load(f)\n","print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"]},{"cell_type":"markdown","metadata":{"id":"1d4b680f"},"source":["## 5. Define Helper Function\n","\n","This simple helper function, `create_messages`, builds the final prompt that will be sent to the LLM. It takes the system prompt, user template, question, and retrieved context, and formats them into the standard list of message objects (`{role: ..., content: ...}`) that the model expects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4hMY5Ucdo4"},"outputs":[],"source":["def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n","    \"\"\"\n","    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n","    \"\"\"\n","    context_text = \"\"\n","    if context:\n","        if isinstance(context, list):\n","            context_text = \"\\n\\n---\\n\\n\".join(context)\n","        elif isinstance(context, str):\n","            context_text = context\n","\n","    # Format the user prompt template\n","    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_content}\n","    ]\n","    return messages"]},{"cell_type":"markdown","metadata":{"id":"76e336e1"},"source":["## 6. Initialize Clients\n","\n","We create the clients for the generative LLM (`FlotorchLLM`) and the Knowledge Base (`FlotorchVectorStore`). These clients will be used inside the loop to get context and generate answers for each question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66f8510"},"outputs":[],"source":["# 1. Set up the LLM for generating answers\n","inference_llm = FlotorchLLM(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    model_id=inference_model_name\n",")\n","\n","# 2. Set up the Knowledge Base connection\n","kb = FlotorchVectorStore(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    vectorstore_id=knowledge_base_repo\n",")\n","\n","# 3. Load prompts into variables\n","system_prompt = prompt_config.get(\"system_prompt\", \"\")\n","user_prompt_template = prompt_config.get(\"user_prompt_template\", \"{question}\")\n","\n","print(\"Models and Knowledge Base are ready.\")"]},{"cell_type":"markdown","metadata":{"id":"f51660d1"},"source":["## 7. Run Experiment Loop\n","\n","This is the core logic for *generating* the answers. We loop through each question in our `ground_truth` list and perform the full RAG pipeline:\n","\n","1.  **Retrieve Context**: We ` kb.search()` to get context from the vector store.\n","2.  **Build Messages**: We use our `create_messages` helper to assemble the final prompt.\n","3.  **Generate Answer**: We `inference_llm.invoke()` to get the agent's response.\n","4.  **Store for Evaluation**: We package all this information into an `EvaluationItem` object and add it to our `evaluation_items` list.\n","\n","We also include a `try...except` block to gracefully handle any errors during the API calls, ensuring the loop doesn't crash.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0e37ac2"},"outputs":[],"source":["evaluation_items = [] # This will store our results\n","\n","# Use simple tqdm for a progress bar\n","print(f\"Running experiment on {len(ground_truth)} items...\")\n","\n","for qa in tqdm(ground_truth):\n","    question = qa.get(\"question\", \"\")\n","    gt_answer = qa.get(\"answer\", \"\")\n","\n","    try:\n","        # --- 1. Retrieve Context ---\n","        search_results = kb.search(query=question)\n","        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n","\n","        # --- 2. Build Messages ---\n","        messages = create_messages(\n","            system_prompt=system_prompt,\n","            user_prompt_template=user_prompt_template,\n","            question=question,\n","            context=context_texts\n","        )\n","\n","        # --- 3. Generate Answer ---\n","        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n","        generated_answer = response.content\n","\n","        # --- 4. Store for Evaluation ---\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=generated_answer,\n","            expected_answer=gt_answer,\n","            context=context_texts, # Store the context for later display\n","            metadata=headers,\n","        ))\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n","        # Store a failure case so we can see it\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=f\"Error: {e}\",\n","            expected_answer=gt_answer,\n","            context=[],\n","            metadata={\"error\": str(e)},\n","        ))\n","\n","print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"]},{"cell_type":"markdown","metadata":{"id":"crt03A2aphia"},"source":["## 8. Initialize the Evaluator\n","\n","With the `evaluation_items` list ready, we can now initialize the `LLMEvaluator`.\n","\n","The `LLMEvaluator` is the central component of the **Flotorch-Eval** library—it orchestrates metric computation, similarity analysis, and LLM-based judgments based on the configuration provided.\n","\n","### Parameter Overview\n","\n","- **`api_key` / `base_url`**  \n","  Credentials required to authenticate and communicate with the Flotorch-Eval service.\n","\n","- **`inferencer_model` / `embedding_model`**  \n","  The evaluator uses:  \n","  - an **LLM** (`inferencer_model`) for reasoning-driven evaluations, and  \n","  - an **embedding model** (`embedding_model`) for semantic similarity and context comparison.\n","\n","- **`evaluation_engine`**  \n","  Set to `\"ragas\"` in this notebook, meaning the evaluator uses the **Ragas** framework, which is well-suited for RAG system evaluation.  \n","  Ragas supports core metrics such as:  \n","  - **Faithfulness**  \n","  - **Answer Relevance**  \n","  - **Context Precision**  \n","  - **Aspect Critic**  \n","\n","  Other available engines include:  \n","  - **`\"deepeval\"`** — Uses the DeepEval framework for LLM-as-judge and critic-based metrics.  \n","  - **`\"auto\"`** — Automatically selects the appropriate engine for each metric.\n","\n","- **`metrics`**  \n","  Defines the metrics to compute—in this case:  \n","  **faithfulness**, **answer_relevance**, **context_precision**, and **aspect_critic**.\n","\n","- **`metric_configs`**  \n","  Provides configuration options for metrics that require additional parameters.  \n","  A key example is **`ASPECT_CRITIC`**, which functions as a *parent metric*.  \n","  Under **Aspect Critic**, you can define one or more evaluation aspects, such as:  \n","  - **hallucination**  (used in this notebook)\n","  - **maliciousness**   \n","  - **toxicity**  \n","  - **instruction adherence**, etc.\n","\n","  Here, we configure Aspect Critic specifically to evaluate **Hallucination**, illustrating how checks for fabricated or unsupported information can be integrated into a RAG evaluation workflow."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5e9f8a1"},"outputs":[],"source":["# Configure a custom metric for hallucination\n","metric_args = {\n","    MetricKey.ASPECT_CRITIC: {\n","        \"hallucination\": {\n","            \"name\": \"hallucination\",\n","            \"definition\": \"Does the answer coming from outof the context?\"\n","        }\n","    }\n","}\n","\n","# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.FAITHFULNESS,\n","        MetricKey.ANSWER_RELEVANCE,\n","        MetricKey.CONTEXT_PRECISION,\n","        MetricKey.ASPECT_CRITIC\n","    ],\n","    metric_configs=metric_args,\n","    evaluation_engine=\"ragas\"\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"a4d3b6f0"},"source":["## 9. Run Evaluation\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the all **ragas** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute  metric scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","source":["### Asynchronous Evaluation"],"metadata":{"id":"JS-b6Osnj24j"}},{"cell_type":"code","source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"],"metadata":{"id":"Ozly29sPgcjR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"],"metadata":{"id":"oDZxEck8j-ia"}},{"cell_type":"code","source":["# print(\"Starting evaluation... This may take a few minutes.\")\n","\n","# eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","# print(\"Evaluation complete.\")"],"metadata":{"id":"YzdLUwxGj9oZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8f1c3a7"},"source":["## 10. View Per-Question Results\n","\n","The `eval_results` variable contains a list of dictionaries, where each dictionary holds\n","the detailed evaluation results and metrics for an individual question.\n","\n","### Process\n","- Pass `eval_results` to the `display_llm_evaluation_results` method.\n","- This renders the results in a clean, structured, and human-readable format.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ppknK5NLISH"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"k8l9m0n1"},"source":["## 11. View Raw JSON Results\n","\n","Finally, we can print the raw `eval_result` list as a formatted JSON. This is useful for seeing all the data at once, including gateway metrics like latency and cost for each individual evaluation call, which are stored inside the `evaluation_metrics` dictionary for each item."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2q3r4s5"},"outputs":[],"source":["print(\"--- Aggregate Evaluation Results ---\")\n","\n","print(json.dumps(eval_results, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"summary_cell_new"},"source":["## 12. Summary\n","\n","This notebook provided a complete, step-by-step workflow for evaluating a RAG agent using Flotorch Eval.\n","\n","We successfully:\n","\n","1.  **Configured** clients for the Flotorch SDK (`FlotorchLLM`, `FlotorchVectorStore`) and Flotorch Eval (`LLMEvaluator`).\n","2.  **Generated** responses by looping through a `gt.json` file, retrieving context from the Knowledge Base, and calling the inference LLM.\n","3.  **Evaluated** each response individually by calling `evaluator_client.evaluate()` for each item, collecting detailed metrics for Faithfulness, Answer Relevancy, Context Precision, and Hallucination.\n","4.  **Displayed** the final, per-question scores in a formatted table, allowing for easy analysis of where the agent is succeeding or failing."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}