{"cells":[{"cell_type":"markdown","metadata":{"id":"9hEI1dQgNg3j"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1QN0rqMopoTvq72H7Y06JIt6Gl0zcLpWx/view?usp=sharing)\n","\n","# Evaluating the Fission Labs Assistant with Flotorch Eval"]},{"cell_type":"markdown","metadata":{"id":"PRAOkCxa9UjX"},"source":["This notebook walks through evaluating a **Fission Labs Assistant** — a retrieval-augmented agent that answers questions about **Fission Labs Organization**.using the **Flotorch SDK** alongside the **Flotorch Eval** library.\n","\n","---\n","\n","### **Use Case Overview**\n","\n","The **Fission Labs Assistant** helps users explore facts about Fission Labs—founded in 2008, headquartered in Sunnyvale, California, with delivery hubs in Hyderabad.  \n","It retrieves relevant information from a **Fission Labs Knowledge Base** built from internal documentation and generates helpful, accurate responses.\n","\n","This notebook focuses on evaluating the **faithfulness** of the model’s answers — that is, whether the generated responses are **factually grounded** in the retrieved context.\n","\n","---\n","\n","### **Notebook Workflow**\n","\n","We’ll follow a structured evaluation process:\n","\n","1. **Iterate Questions** – Loop through each question in the `gt.json` file (Ground Truth).  \n","2. **Retrieve Context** – Fetch relevant passages from the Fission Labs Knowledge Base.  \n","3. **Generate Answer** – Use the system prompt and LLM to produce a response.  \n","4. **Store Results** – Log each question, retrieved context, generated answer, and ground truth.  \n","5. **Evaluate Faithfulness** – Use `LLMEvaluator` from Flotorch Eval to assess how well each response is grounded in the given context.  \n","6. **Display Results** – Summarize the faithfulness scores in a simple comparison table.\n","\n","---\n","\n","### **Metric Evaluated — Faithfulness**\n","\n","We focus on a single quality signal: **Faithfulness**. It measures how faithfully a generated answer reflects the supporting context retrieved from our knowledge base. A high score means every claim in the answer is grounded in the evidence; a low score indicates hallucinations or contradictions.\n","\n","#### Ragas Faithfulness (Flotorch `evaluation_engine=\"ragas\"`)\n","According to the [Ragas documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/), the pipeline:\n","- breaks the generated answer into atomic claims using an evaluator LLM,\n","- checks each claim against retrieved context passages for support, and\n","- reports the ratio of supported claims to total claims as a score between 0.0 and 1.0.\n","\n","#### DeepEval Faithfulness (Flotorch `evaluation_engine=\"deepeval\"`)\n","Per the [DeepEval faithfulness specification](https://deepeval.com/docs/metrics-faithfulness), the evaluator:\n","- extracts fine-grained statements from the answer,\n","- verifies each statement against the provided context using DeepEval’s judging rubric,\n","- outputs a 0.0–1.0 score, and\n","- optionally applies a threshold (for example `0.8`) to automate pass/fail checks or generate reasoning about unsupported claims.\n","\n","Both engines serve the same goal—ensuring generated answers stay truthful—but they offer different tuning knobs. Ragas emphasizes retrieval-aware scoring, while DeepEval adds thresholding and richer diagnostics for production monitoring.\n","\n","---\n","\n","### **Evaluation Engine**\n","\n","- `evaluation_engine=\"auto\"` — lets Flotorch Eval mix Ragas and DeepEval according to the priority routing described in the [flotorch-eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop) (Ragas first, DeepEval as fallback) so that faithfulness, answer relevance, and context metrics always run on the best available backend.\n","- `evaluation_engine=\"ragas\"` — keeps every metric inside the [**Ragas**](https://docs.ragas.io/en/stable/getstarted/) rubric for retrieval-aware evaluations (faithfulness, answer relevance, context precision, aspect critic, etc.), which is what we configure in the first half of this notebook.\n","- `evaluation_engine=\"deepeval\"` — routes metrics through DeepEval’s engine (faithfulness, answer relevancy, context relevancy, context precision, context recall, hallucination) while still capturing Flotorch gateway telemetry as documented in the [Flotorch Eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop). This mode is showcased later in the notebook.\n","\n","We begin with the Ragas-only flow and then repeat the run with the DeepEval engine to highlight the differences in scoring and configuration.  \n","\n","---\n","\n","### **Requirements**\n","\n","- Flotorch account with configured LLM, embedding model, and Knowledge Base.  \n","- `gt.json` containing question–answer pairs for evaluation.  \n","- `prompt.json` containing the system and user prompt templates.  \n","---\n","\n","#### **Documentation References**\n","- [**flotorch-eval GitHub repo**](https://github.com/FissionAI/flotorch-eval/tree/develop) — reference implementation with sample notebooks and evaluation pipelines.\n","- [**Ragas Faithfulness Documentation**](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) — detailed explanation of the metric.\n","- [**DeepEval Faithfulness Documentation**](https://deepeval.com/docs/metrics-faithfulness) — detailed explanation of the metric.\n","---"]},{"cell_type":"markdown","metadata":{"id":"e2a09f3e"},"source":["## 1. Install Dependencies\n","\n","First, we install the two necessary libraries. The `-q` flag is for a \"quiet\" installation, hiding the lengthy output.\n","\n","-   `flotorch`: The main Python SDK for interacting with all Flotorch services, including LLMs and Knowledge Bases.\n","-   `flotorch-eval[llm]`: The evaluation library. We use the `[llm]` extra to install dependencies required for LLM-based (Ragas) evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwnpCfiUXfJ_"},"outputs":[],"source":["# Install flotorch-sdk and flotorch-core\n","# You can safely ignore dependency errors during installation.\n","%pip install flotorch==3.1.0b1 flotorch-eval==2.0.0\n","%pip install opentelemetry-instrumentation-httpx==0.58b0"]},{"cell_type":"markdown","metadata":{"id":"e4418652"},"source":["## 2. Configure Environment\n","\n","This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n","\n","-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n","-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n","-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n","-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n","-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n","-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent.\n","\n","### Example :\n","\n","| Parameter | Description | Example |\n","|-----------|-------------|---------|\n","| `FLOTORCH_API_KEY` | Your API authentication key | `sk_...` |\n","| `FLOTORCH_BASE_URL` | Gateway endpoint | `https://gateway.flotorch.cloud` |\n","| `inference_model_name` | The LLM your agent uses to generate answers | `flotorch/gpt-4o-mini` |\n","| `evaluation_llm_model_name` | The LLM used to evaluate the answers | `flotorch/gpt-4o` |\n","| `evaluation_embedding_model_name` | Embedding model for semantic similarity checks | `open-ai/text-embedding-ada-002` |\n","| `knowledge_base_repo` | The ID of your Flotorch Knowledge Base | `digital-twin` |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkEYxQ9yRmtQ"},"outputs":[],"source":["import getpass  # Securely prompt without echoing in Prefect/notebooks\n","\n","# Prefect-side authentication for Flotorch access\n","try:\n","    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  # Used by Prefect flow and local runs\n","    print(f\"Success\")\n","except getpass.GetPassWarning as e:\n","    print(f\"Warning: {e}\")\n","    FLOTORCH_API_KEY = \"\"\n","\n","FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint\n","\n","inference_model_name = \"flotorch/<your-model-name>\"  # Model generating answers\n","evaluation_llm_model_name = \"flotorch/<your_model_name>\"  # Model judging answer quality\n","evaluation_embedding_model_name = \"flotorch/<embedding_model_name>\"  # Embedding model for similarity checks\n","\n","knowledge_base_repo = \"<your_knowledge_base_id>\""]},{"cell_type":"markdown","metadata":{"id":"76978434"},"source":["## 3. Import Required Libraries\n","\n","### Purpose\n","Import all required components for evaluating the RAG assistant.\n","\n","### Key Components\n","- `json` : Loads configuration files and ground truth data from disk\n","- `tqdm` : Shows a lightweight progress bar while iterating over evaluation items\n","- `FlotorchLLM` : Connects to the Flotorch inference endpoint for answer generation\n","- `FlotorchVectorStore` : Retrieves context snippets from the configured knowledge base\n","- `memory_utils` : Utility helpers for extracting text from vector-store search results\n","- `LLMEvaluator`, EvaluationItem, MetricKey** : Runs metric scoring for the generated answers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RRnnoQfXbOM"},"outputs":[],"source":["#Required imports\n","import json\n","from typing import List\n","from tqdm import tqdm # Use standard tqdm for simple progress bars\n","from google.colab import files\n","\n","# Flotorch SDK components\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.memory import FlotorchVectorStore\n","from flotorch.sdk.utils import memory_utils\n","\n","# Flotorch Eval components\n","from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n","from flotorch_eval.llm_eval import display_llm_evaluation_results\n","\n","\n","print(\"Imported necessary libraries successfully\")"]},{"cell_type":"markdown","metadata":{"id":"8E6DGAqY7nPS"},"source":["## 4. Load Data and Prompts\n","\n","### Purpose\n","Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local\n","files.\n","\n","### Files Required\n","\n","**1. `gt.json` (Ground Truth)**\n","Contains question-answer pairs for evaluation. Each `answer` is the expected correct response.\n","\n","```json\n","[\n","  {\n","    \"question\": \"When was Fission Labs founded?\",\n","    \"answer\": \"Fission Labs was founded in 2008.\"\n","  },\n","  {\n","    \"question\": \"Who is the Co-Founder and CEO of Fission Labs?\",\n","    \"answer\": \"Eswar Lingam is the Co-Founder and CEO of Fission Labs.\"\n","  }\n","]\n","```\n","\n","**2. `prompt.json` (Agent Prompts)**\n","Defines the system prompt and user prompt template with `{context}` and `{question}` placeholders for dynamic formatting.\n","\n","```json\n","{\n","  \"system_prompt\": \"You are a helpful Fission labs assistant. Answer based only on the context provided.\",\n","  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n","}\n","```\n","\n","### Instructions\n","Update `gt_path` and `prompt_path` variables in the next cell to point to your local file locations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFYSNDJo1MAP"},"outputs":[],"source":["print(\"Please upload your Ground Truth file (gt.json)\")\n","gt_upload = files.upload()\n","\n","gt_path = list(gt_upload.keys())[0]\n","with open(gt_path, 'r') as f:\n","    ground_truth = json.load(f)\n","print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n","\n","\n","print(\"Please upload your Prompts file (prompts.json)\")\n","prompts_upload = files.upload()\n","\n","prompts_path = list(prompts_upload.keys())[0]\n","with open(prompts_path, 'r') as f:\n","    prompt_config = json.load(f)\n","print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"]},{"cell_type":"markdown","metadata":{"id":"1d4b680f"},"source":["## 5. Define Helper Function\n","\n","### Purpose\n","Create a prompt-formatting helper for LLM message construction.\n","\n","### Functionality\n","The `create_messages` function:\n","- Builds the final prompt that will be sent to the LLM.\n","- Accepts system prompt, user prompt template, question, and retrieved context chunks\n","- Replaces `{context}` and `{question}` placeholders in the user prompt\n","- Returns a structured message list with (`{role: ..., content: ...}`) fields ready for LLM consumption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4hMY5Ucdo4"},"outputs":[],"source":["def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n","    \"\"\"\n","    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n","    \"\"\"\n","    context_text = \"\"\n","    if context:\n","        if isinstance(context, list):\n","            context_text = \"\\n\\n---\\n\\n\".join(context)\n","        elif isinstance(context, str):\n","            context_text = context\n","\n","    # Format the user prompt template\n","    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_content}\n","    ]\n","    return messages\n"]},{"cell_type":"markdown","metadata":{"id":"76e336e1"},"source":["## 6. Initialize Clients\n","\n","### Purpose\n","Set up the infrastructure for RAG pipeline execution.\n","\n","### Components Initialized\n","1. **FlotorchLLM** (`inference_llm`): Connects to the LLM endpoint for generating answers based on retrieved context\n","2. **FlotorchVectorStore** (`kb`): Connects to the Knowledge Base for semantic search and context retrieval\n","3. **Prompt Variables**: Extracts system prompt and user prompt template from `prompt_config` for dynamic message formatting\n","\n","These clients power the evaluation loop by retrieving relevant context and generating answers for each question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66f8510"},"outputs":[],"source":["# 1. Set up the LLM for generating answers\n","inference_llm = FlotorchLLM(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    model_id=inference_model_name\n",")\n","\n","# 2. Set up the Knowledge Base connection\n","kb = FlotorchVectorStore(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    vectorstore_id=knowledge_base_repo\n",")\n","\n","# 3. Load prompts into variables\n","system_prompt = prompt_config.get(\"system_prompt\", \"\")\n","user_prompt_template = prompt_config.get(\"user_prompt\", \"{question}\")\n","\n","print(\"Models and Knowledge Base are ready.\")"]},{"cell_type":"markdown","metadata":{"id":"f51660d1"},"source":["## 7. Run Experiment Loop\n","\n","### Purpose\n","Execute the full RAG pipeline for each question to generate answers for evaluation.\n","\n","### Pipeline Steps\n","For each question in `ground_truth`, the loop performs:\n","\n","1. **Retrieve Context**: Searches the Knowledge Base (`kb.search()`) to fetch relevant context passages\n","2. **Build Messages**: Uses `create_messages()` to format the system prompt, user prompt, question, and retrieved context into LLM-ready messages\n","3. **Generate Answer**: Invokes the inference LLM (`inference_llm.invoke()`) with `return_headers=True` to capture response metadata (cost, latency, tokens)\n","4. **Store for Evaluation**: Packages question, generated answer, expected answer, context, and metadata into an `EvaluationItem` object\n","\n","### Error Handling\n","A `try...except` block gracefully handles API failures, storing error messages as evaluation items to ensure the loop completes without crashes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0e37ac2"},"outputs":[],"source":["evaluation_items = [] # This will store our results\n","\n","# Use simple tqdm for a progress bar\n","print(f\"Running experiment on {len(ground_truth)} items...\")\n","\n","for qa in tqdm(ground_truth):\n","    question = qa.get(\"question\", \"\")\n","    gt_answer = qa.get(\"answer\", \"\")\n","\n","    try:\n","        # --- 1. Retrieve Context ---\n","        search_results = kb.search(query=question)\n","        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n","\n","        # --- 2. Build Messages ---\n","        messages = create_messages(\n","            system_prompt=system_prompt,\n","            user_prompt_template=user_prompt_template,\n","            question=question,\n","            context=context_texts\n","        )\n","\n","        # --- 3. Generate Answer ---\n","        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n","        generated_answer = response.content\n","\n","        # --- 4. Store for Evaluation ---\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=generated_answer,\n","            expected_answer=gt_answer,\n","            context=context_texts, # Store the context for later display\n","            metadata=headers,\n","        ))\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n","        # Store a failure case so we can see it\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=f\"Error: {e}\",\n","            expected_answer=gt_answer,\n","            context=[],\n","            metadata={\"error\": str(e)},\n","        ))\n","\n","print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"]},{"cell_type":"markdown","metadata":{"id":"yrEMDjEJ9Ujj"},"source":["# Flotorch-Eval Has Both Ragas and DeepEval Support\n","1. `flotorch-eval` ships Ragas and DeepEval engines together, letting auto mode pick the right one per the [docs](https://github.com/FissionAI/flotorch-eval/tree/develop).\n","2. Ragas stays primary for faithfulness, answer relevance, and context precision when embeddings are configured.\n","3. DeepEval backs up the same metrics whenever Ragas prerequisites are missing, so coverage never drops.\n","4. Gateway telemetry (latency, cost, tokens) remains active because auto mode still passes Flotorch headers through each run.\n"]},{"cell_type":"markdown","metadata":{"id":"crt03A2aphia"},"source":["## 8. Initialize the Evaluator\n","\n","### Using Ragas Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can set up the `LLMEvaluator`.\n","\n","This class is the core component of the **Flotorch-Eval** library — think of it as the *“head judge”* for our evaluation process. It coordinates metric calculations, semantic comparisons, and LLM-based judgments using the configuration we provide.\n","\n","### Parameter Insights\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — The evaluator uses:\n","  - an **LLM** (`inferencer_model`) for reasoning-based checks, and  \n","  - an **embedding model** (`embedding_model`) for semantic and contextual similarity evaluations.  \n","- **`evaluation_engine`** — Here, we set this to `\"ragas\"`, meaning the evaluator will use the **[Ragas framework](https://docs.ragas.io/en/stable/getstarted/)** for metric computation.  \n","  Ragas is well-suited for RAG-style evaluations and handles metrics such as:\n","  - **Faithfulness**\n","  - **Answer Relevance**\n","  - **Context Precision**\n","  - **Aspect Critic (custom maliciousness check)**\n","- **`metrics`** — In this configuration, we evaluate only **`MetricKey.FAITHFULNESS`**.\n","\n","### Faithfulness Metric\n","\n","**Definition**: evaluates how factually consistent a generated response is with the retrieved context. It measures whether all claims in the generated answer can be supported by the context. The score ranges from 0 to 1, calculated as: **Number of claims supported by context / Total number of claims in the response**. This metric is crucial for preventing hallucinations and ensuring the AI doesn't fabricate information beyond what's provided in the source documents.\n","\n","**How It Works**:\n","1. Breaks the answer into individual claims\n","2. Checks each claim against retrieved context  \n","3. Score = (Supported claims) / (Total claims)\n","\n","**Example**:\n","\n","*Context*: \"Fission Labs was founded in 2008 and is headquartered in Sunnyvale, California.\"\n","\n","*Faithful Answer* (Correct): \"Fission Labs was founded in 2008 and is headquartered in Sunnyvale, California.\" → **Score: 1.0**\n","\n","*Unfaithful Answer* (Incorrect): \"Fission Labs was founded in 2005 and has over 1000 employees.\" → **Score: 0.0**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5e9f8a1"},"outputs":[],"source":["# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.FAITHFULNESS,\n","    ],\n","    evaluation_engine=\"ragas\"\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"a4d3b6f0"},"source":["## 9. Run Evaluation (Ragas)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **faithfulness** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **faithfulness** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Faithfulness scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute faithfulness scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"mEw-9pvWOPNU"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1d0e3b2"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"2Im2PIQiOPNU"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"88qnUGWLOPNV"},"outputs":[],"source":["# print(\"Starting evaluation... This may take a few minutes.\")\n","\n","# eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","# print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"e8f1c3a7"},"source":["## 10. View Per-Question Results\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9g2h4j6"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"mcjSz0jm9Ujm"},"source":["# 11. Initialize the Evaluator\n","\n","### Using DeepEval Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can switch the `LLMEvaluator` to the **DeepEval** backend to compare its scoring with the Ragas run.\n","\n","This class is still the *“head judge”* for our evaluation process; we’re simply changing which judging rubric it follows.\n","\n","### Parameter Insights (DeepEval Mode)\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — DeepEval-powered scoring still needs the evaluator LLM and embeddings for semantic checks.  \n","- **`evaluation_engine=\"deepeval\"`** — Routes metrics through DeepEval, which (per the [flotorch-eval repository](https://github.com/FissionAI/flotorch-eval/tree/develop)) unlocks the following metric keys:\n","  - **`MetricKey.FAITHFULNESS`**\n","  - **`MetricKey.ANSWER_RELEVANCY`**\n","  - **`MetricKey.CONTEXT_RELEVANCY`**\n","  - **`MetricKey.CONTEXT_PRECISION`**\n","  - **`MetricKey.CONTEXT_RECALL`**\n","  - **`MetricKey.HALLUCINATION`**\n","  These are the same metrics surfaced in Flotorch’s *auto* mode when Ragas prerequisites (like embeddings) are missing.\n","- **`metric_configs`** — Lets us pass DeepEval-specific arguments such as custom thresholds for faithfulness (we set `0.8` below) and tuning knobs for other metrics (maliciousness, hallucination, etc.).\n","\n","### DeepEval Faithfulness Metric\n","\n","**Definition**: checks whether every claim in the generated answer is supported by the supplied context using the [DeepEval faithfulness rubric](https://deepeval.com/docs/metrics-faithfulness). The metric returns a score between 0 and 1, where higher values indicate fully grounded answers that satisfy the configured threshold.\n","\n","**How It Works**:\n","1. Extracts atomic statements from the model answer with DeepEval’s grading LLM.\n","2. Verifies each statement against the provided context to classify it as supported or unsupported, optionally producing reasoning for any mismatch.\n","3. Computes the faithfulness score as supported statements ÷ total statements, then compares the result to any threshold set in `metric_configs` (e.g., `0.8`) to drive pass/fail signals.\n","\n","**Example**:\n","\n","*Context*: \"Fission Labs was founded in 2008, is headquartered in Sunnyvale, California, and runs delivery hubs in Hyderabad, India.\"\n","\n","*Faithful Answer* (Score ≈ 1.0): \"Fission Labs was founded in 2008 in Sunnyvale and operates delivery hubs in Hyderabad.\" — every statement is grounded, so the answer clears the 0.8 threshold.\n","\n","*Unfaithful Answer* (Score ≈ 0.0): \"Fission Labs launched in 2012, moved its headquarters to Austin, and recently opened a hardware division.\" — unsupported claims drive the score to 0, and DeepEval surfaces reasoning indicating the contradictions.\n","\n","This mirrors the Ragas faithfulness workflow while adding threshold-driven outcomes and optional explanations that are useful for automated guardrails."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hknxbG9Z9Ujm"},"outputs":[],"source":["# Configure a custom metric for faithfulness\n","metric_args = {\n","\n","    \"faithfulness\": {\n","        \"threshold\": 0.7\n","        }\n","\n","}\n","\n","# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.FAITHFULNESS,\n","    ],\n","    evaluation_engine=\"deepeval\",\n","    metric_configs=metric_args\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"tIDzbV0w9Ujm"},"source":["## 12. Run Evaluation (DeepEval)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **faithfulness** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **faithfulness** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Faithfulness scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute faithfulness scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"DHSgxBKCOPNW"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g6wP0caU9Ujn"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"XxMLtDOeOPNX"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkJ6iO7eOPNX"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"dLcqiwrZ9Ujn"},"source":["## 13. View Per-Question Results\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R_hpcTrb9Ujn"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"k8l9m0n1"},"source":["## 14. View Raw JSON Results\n","\n","### Purpose\n","Display the complete evaluation results in JSON format for detailed inspection and programmatic access.\n","\n","### Output Structure\n","The JSON output includes for each question:\n","- **model**: The evaluation LLM model used\n","- **input_query**: The original question\n","- **context**: Full retrieved context passages (not truncated)\n","- **generated_answer**: Complete LLM-generated response\n","- **groundtruth_answer**: Expected correct answer\n","- **evaluation_metrics**: Dictionary containing:\n","  - **faithfulness**: Faithfulness score (0.0 to 1.0)\n","  - **average_score**: Average of all evaluated metrics\n","  - **total_latency_ms**: Total evaluation time in milliseconds\n","  - **total_cost**: Cost of evaluation in USD\n","  - **total_tokens**: Token count for evaluation\n","\n","This raw JSON format is useful for further analysis, exporting results, or integrating with other tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2q3r4s5"},"outputs":[],"source":["print(\"--- Aggregate Evaluation Results ---\")\n","print(json.dumps(eval_results, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"summary_cell_new"},"source":["## 15. Summary\n","\n","### What We Accomplished\n","\n","This notebook provided a complete, step-by-step workflow for evaluating a RAG agent using Flotorch Eval with the Ragas faithfulness metric.\n","\n","### Workflow Summary\n","\n","1. **Configured Infrastructure**\n","   - Set up `FlotorchLLM` for answer generation\n","   - Connected to `FlotorchVectorStore` for context retrieval\n","   - Initialized `LLMEvaluator` with Ragas engine for faithfulness scoring\n","\n","2. **Generated Responses**\n","   - Loaded ground truth questions from `gt.json`\n","   - Retrieved relevant context from the Knowledge Base for each question\n","   - Generated answers using the inference LLM with retrieved context\n","   - Captured metadata (cost, latency, tokens) from each LLM call\n","\n","3. **Evaluated Faithfulness**\n","   - Scored each generated answer using the Ragas faithfulness metric\n","   - Verified that answers are factually grounded in retrieved context\n","   - Collected evaluation metrics and gateway statistics for each question\n","\n","4. **Visualized Results**\n","   - Displayed per-question scores in a formatted table for quick analysis\n","   - Exported complete results as JSON for further processing\n","   - Observed that all real-world answers scored 1.0 (perfect faithfulness)\n","\n","### Key Takeaways\n","\n","- **Faithfulness = 1.0** means generated answers are fully supported by the context (no hallucinations)\n","- **Faithfulness = 0.0** means generated answers contain unsupported or contradictory claims\n","- The metric evaluates **Generated Answer ↔ Context**, NOT **Generated Answer ↔ Ground Truth**\n","- Faithfulness is crucial for ensuring RAG systems produce reliable, context-grounded responses without hallucinations—critical for applications like the Fission Labs Assistant where accuracy is paramount\n","\n","### Ragas vs. DeepEval in Practice\n","\n","| Dimension | Ragas Faithfulness | DeepEval Faithfulness |\n","|-----------|--------------------|------------------------|\n","| **Docs** | [Metric docs](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) | [Metric docs](https://deepeval.com/docs/metrics-faithfulness) |\n","| **Core Flow** | Splits answers into claims and checks each one against retrieved passages; score is supported-claims ÷ total claims (0–1). | Similar claim analysis, with optional reasoning text and configurable thresholds for pass/fail automation. |\n","| **Engine Toggle** | `evaluation_engine=\"ragas\"` keeps all scoring inside the Ragas framework. | `evaluation_engine=\"deepeval\"` enables DeepEval metrics per the [Flotorch Eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop). |\n","| **Additional Metrics** | Faithfulness, answer relevance, context precision, aspect critic (when configured). | Faithfulness, answer relevancy, context relevancy, context precision, context recall, hallucination, plus threshold-based outcomes. |\n","\n","**Example Context**: “Fission Labs was founded in 2008 and operates delivery hubs in Hyderabad.”\n","\n","- **Ragas Outcome**\n","  - Faithful answer: “Founded in 2008 with delivery hubs in Hyderabad” → score ≈ 1.0\n","  - Unfaithful answer: “Founded in 2008 with offices in New York” → unsupported claim lowers score toward 0\n","- **DeepEval Outcome**\n","  - Faithful answer: passes when threshold is 0.8, no hallucinations flagged\n","  - Unfaithful answer: fails threshold, DeepEval surfaces reasoning showing the “New York” claim isn’t in the context\n","\n","DeepEval’s broader metric set complements the Ragas run, giving both retrieval-focused and hallucination-aware views of answer quality.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}