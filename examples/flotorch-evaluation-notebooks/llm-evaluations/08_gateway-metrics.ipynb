{"cells":[{"cell_type":"markdown","metadata":{"id":"kE624-6Rw0cu"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/17nIj3kRgNnJD4Gu0OUavGxbpqa-RefSd/view?usp=sharing)\n","\n","# Gateway Metrics Evaluation with Flotorch Eval\n","\n","This notebook demonstrates how to track and evaluate **gateway metrics** (latency, cost, token usage) using the **Flotorch SDK** alongside the **Flotorch Eval** library. Gateway metrics are automatically tracked by the Flotorch Gateway for every LLM call, providing essential operational insights for performance monitoring, cost management, and usage analytics.\n","\n","---\n","\n","### **Use Case Overview**\n","\n","The **Agriculture & Farming Assistant** helps users explore comprehensive agriculture and farming knowledge covering:\n","\n","**Core Agricultural Topics:**\n","- **Types of Agricultural Systems**\n","- **Major Crop Production**\n","- **Livestock Management**:\n","- **Soil Management**:\n","- **Irrigation and Water Management**:\n","- **Pest and Disease Management**:\n","- **Sustainable Agriculture**:\n","\n","This notebook focuses on evaluating the **faithfulness** of the model’s answers — that is, whether the generated responses are **factually grounded** in the retrieved context.\n","\n","---\n","\n","### **Notebook Workflow**\n","\n","We’ll follow a structured evaluation process:\n","\n","1. **Iterate Questions** – Loop through each question in the `gt.json` file (Ground Truth).  \n","2. **Retrieve Context** – Fetch relevant passages from the Knowledge Base.  \n","3. **Generate Answer** – Use the system prompt and LLM to produce a response.  \n","4. **Store Results** – Log each question, retrieved context, generated answer, ground truth, and **gateway metrics (latency, cost, tokens)** from response headers.  \n","5. **Evaluate Metrics** – Use `LLMEvaluator` from Flotorch Eval to assess response quality and extract gateway metrics.  \n","6. **Display Results** – Summarize **gateway metrics** (latency, cost, token usage) and evaluation scores in comparison tables and analysis.\n","\n","---\n","\n","### **Metrics Evaluated**\n","\n","#### **1. Gateway Metrics (Primary Focus)**\n","\n","**Gateway metrics** are automatically tracked by the Flotorch Gateway for every LLM call. These metrics provide essential operational insights:\n","\n","| Metric | Source | Description | Value |\n","|--------|--------|-------------|-------|\n","| **LATENCY** | Gateway | Measures total and average latency across LLM calls | Milliseconds (ms) |\n","| **COST** | Gateway | Tracks total cost of LLM operations | USD ($) |\n","| **TOKEN_USAGE** | Gateway | Monitors total token consumption | Token count |\n","\n","**How Gateway Metrics Work:**\n","\n","1. When you call `FlotorchLLM.invoke()` with `return_headers=True`, the gateway returns response headers containing:\n","   - Request/response latency information\n","   - Cost breakdown per operation\n","   - Token usage (input tokens + output tokens)\n","\n","2. These headers are automatically passed as `metadata` to `EvaluationItem`, and Flotorch Eval extracts gateway metrics from them.\n","\n","3. Gateway metrics are computed automatically - **no additional configuration required**. Simply pass the headers as metadata and the evaluator handles the rest.\n","\n","**Why Gateway Metrics Matter:**\n","\n","- **Performance Monitoring**: Track latency to identify bottlenecks and optimize response times\n","- **Cost Management**: Monitor spending to budget and optimize model usage\n","- **Usage Analytics**: Understand token consumption patterns to plan capacity\n","\n","For more details, see the [Flotorch Eval documentation](https://github.com/FissionAI/flotorch-eval/tree/develop).\n","\n","#### **2. Faithfulness Metric**\n","\n","We evaluate **Faithfulness** to measure how faithfully a generated answer reflects the supporting context retrieved from our knowledge base. A high score means every claim in the answer is grounded in the evidence; a low score indicates hallucinations or contradictions.\n","\n","#### Ragas Faithfulness (Flotorch `evaluation_engine=\"ragas\"`)\n","According to the [Ragas documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/), the pipeline:\n","- breaks the generated answer into atomic claims using an evaluator LLM,\n","- checks each claim against retrieved context passages for support, and\n","- reports the ratio of supported claims to total claims as a score between 0.0 and 1.0.\n","\n","---\n","\n","### **Evaluation Engine**\n","\n","- `evaluation_engine=\"ragas\"` — keeps every metric inside the [**Ragas**](https://docs.ragas.io/en/stable/getstarted/) rubric for retrieval-aware evaluations (faithfulness, answer relevance, context precision, aspect critic, etc.), which is what we configure in the this notebook.\n","\n","\n","---\n","\n","### **Requirements**\n","\n","- Flotorch account with configured LLM, embedding model, and Knowledge Base.  \n","- `gt.json` containing question–answer pairs for evaluation.  \n","- `prompt.json` containing the system and user prompt templates.  \n","---\n","\n","#### **Documentation References**\n","- [**flotorch-eval GitHub repo**](https://github.com/FissionAI/flotorch-eval/tree/develop) — reference implementation with sample notebooks and evaluation pipelines.\n","- [**Ragas Faithfulness Documentation**](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/faithfulness/) — detailed explanation of the metric.\n","---"]},{"cell_type":"markdown","metadata":{"id":"e2a09f3e"},"source":["## 1. Install Dependencies\n","\n","First, we install the two necessary libraries. The `-q` flag is for a \"quiet\" installation, hiding the lengthy output.\n","\n","-   `flotorch`: The main Python SDK for interacting with all Flotorch services, including LLMs and Knowledge Bases.\n","-   `flotorch-eval[llm]`: The evaluation library. We use the `[llm]` extra to install dependencies required for LLM-based (Ragas) evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwnpCfiUXfJ_"},"outputs":[],"source":["# Install flotorch-sdk and flotorch-core\n","# You can safely ignore dependency errors during installation.\n","\n","%pip install flotorch==3.1.0b1 flotorch-eval==2.0.0\n","%pip install opentelemetry-instrumentation-httpx==0.58b0"]},{"cell_type":"markdown","metadata":{"id":"e4418652"},"source":["## 2. Configure Environment\n","\n","This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n","\n","-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n","-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n","-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n","-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n","-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n","-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent.\n","\n","### Example :\n","\n","| Parameter | Description | Example |\n","|-----------|-------------|---------|\n","| `FLOTORCH_API_KEY` | Your API authentication key | `sk_...` |\n","| `FLOTORCH_BASE_URL` | Gateway endpoint | `https://gateway.flotorch.cloud` |\n","| `inference_model_name` | The LLM your agent uses to generate answers | `flotorch/gpt-4o-mini` |\n","| `evaluation_llm_model_name` | The LLM used to evaluate the answers | `flotorch/gpt-4o` |\n","| `evaluation_embedding_model_name` | Embedding model for semantic similarity checks | `open-ai/text-embedding-ada-002` |\n","| `knowledge_base_repo` | The ID of your Flotorch Knowledge Base | `agriculture-farming` |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkEYxQ9yRmtQ"},"outputs":[],"source":["import getpass  # Securely prompt without echoing in Prefect/notebooks\n","\n","# Prefect-side authentication for Flotorch access\n","try:\n","    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  # Used by Prefect flow and local runs\n","    print(f\"Success\")\n","except getpass.GetPassWarning as e:\n","    print(f\"Warning: {e}\")\n","    FLOTORCH_API_KEY = \"\"\n","\n","FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint\n","\n","inference_model_name = \"flotorch/<your-model-name>\"  # Model generating answers\n","evaluation_llm_model_name = \"flotorch/<your_model_name>\"  # Model judging answer quality\n","evaluation_embedding_model_name = \"flotorch/<embedding_model_name>\"  # Embedding model for similarity checks\n","\n","knowledge_base_repo = \"<your_knowledge_base_id>\" #Knowledge_base ID"]},{"cell_type":"markdown","metadata":{"id":"76978434"},"source":["## 3. Import Required Libraries\n","\n","### Purpose\n","Import all required components for evaluating the RAG assistant.\n","\n","### Key Components\n","- `json` : Loads configuration files and ground truth data from disk\n","- `tqdm` : Shows a lightweight progress bar while iterating over evaluation items\n","- `FlotorchLLM` : Connects to the Flotorch inference endpoint for answer generation\n","- `FlotorchVectorStore` : Retrieves context snippets from the configured knowledge base\n","- `memory_utils` : Utility helpers for extracting text from vector-store search results\n","- `LLMEvaluator`, EvaluationItem, MetricKey** : Runs metric scoring for the generated answers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RRnnoQfXbOM"},"outputs":[],"source":["#Required imports\n","import json\n","from typing import List\n","from tqdm import tqdm # Use standard tqdm for simple progress bars\n","from google.colab import files\n","\n","# Flotorch SDK components\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.memory import FlotorchVectorStore\n","from flotorch.sdk.utils import memory_utils\n","\n","# Flotorch Eval components\n","from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n","from flotorch_eval import display_llm_evaluation_results\n","\n","print(\"Imported necessary libraries successfully\")"]},{"cell_type":"markdown","metadata":{"id":"8E6DGAqY7nPS"},"source":["## 4. Load Data and Prompts\n","\n","### Purpose\n","Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local\n","files.\n","\n","### Files Required\n","\n","**1. `gt.json` (Ground Truth)**  \n","Contains question-answer pairs for evaluation. Each `answer` is the expected correct response.\n","\n","```json\n","[\n","  {\n","    \"question\": \"What percentage of global greenhouse gas emissions does agriculture account for?\",\n","    \"answer\": \"Agriculture accounts for approximately 10 to 12 percent of global greenhouse gas emissions directly and additional emissions through associated land use changes.\"\n","  },\n","  {\n","    \"question\": \"What percentage of global agricultural land is irrigated and how much food does it produce?\",\n","    \"answer\": \"Approximately 20 percent of global agricultural land is irrigated, yet this land produces about 40 percent of the world's food.\"\n","  }\n","]\n","```\n","\n","**2. `prompt.json` (Agent Prompts)**  \n","Defines the system prompt and user prompt template with `{context}` and `{question}` placeholders for dynamic formatting.\n","\n","```json\n","{\n","  \"system_prompt\": \"You are a helpful agriculture and farming assistant. Answer based only on the context provided.\",\n","  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n","}\n","```\n","\n","### Instructions\n","Update `gt_path` and `prompt_path` variables in the next cell to point to your local file locations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFYSNDJo1MAP"},"outputs":[],"source":["print(\"Please upload your Ground Truth file (gt.json)\")\n","gt_upload = files.upload()\n","\n","gt_path = list(gt_upload.keys())[0]\n","with open(gt_path, 'r') as f:\n","    ground_truth = json.load(f)\n","print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n","\n","\n","print(\"Please upload your Prompts file (prompts.json)\")\n","prompts_upload = files.upload()\n","\n","prompts_path = list(prompts_upload.keys())[0]\n","with open(prompts_path, 'r') as f:\n","    prompt_config = json.load(f)\n","print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"]},{"cell_type":"markdown","metadata":{"id":"1d4b680f"},"source":["## 5. Define Helper Function\n","\n","### Purpose\n","Create a prompt-formatting helper for LLM message construction.\n","\n","### Functionality\n","The `create_messages` function:\n","- Builds the final prompt that will be sent to the LLM.\n","- Accepts system prompt, user prompt template, question, and retrieved context chunks\n","- Replaces `{context}` and `{question}` placeholders in the user prompt\n","- Returns a structured message list with (`{role: ..., content: ...}`) fields ready for LLM consumption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4hMY5Ucdo4"},"outputs":[],"source":["def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n","    \"\"\"\n","    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n","    \"\"\"\n","    context_text = \"\"\n","    if context:\n","        if isinstance(context, list):\n","            context_text = \"\\n\\n---\\n\\n\".join(context)\n","        elif isinstance(context, str):\n","            context_text = context\n","\n","    # Format the user prompt template\n","    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_content}\n","    ]\n","    return messages\n"]},{"cell_type":"markdown","metadata":{"id":"76e336e1"},"source":["## 6. Initialize Clients\n","\n","### Purpose\n","Set up the infrastructure for RAG pipeline execution.\n","\n","### Components Initialized\n","1. **FlotorchLLM** (`inference_llm`): Connects to the LLM endpoint for generating answers based on retrieved context\n","2. **FlotorchVectorStore** (`kb`): Connects to the Knowledge Base for semantic search and context retrieval\n","3. **Prompt Variables**: Extracts system prompt and user prompt template from `prompt_config` for dynamic message formatting\n","\n","These clients power the evaluation loop by retrieving relevant context and generating answers for each question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66f8510"},"outputs":[],"source":["# 1. Set up the LLM for generating answers\n","inference_llm = FlotorchLLM(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    model_id=inference_model_name\n",")\n","\n","# 2. Set up the Knowledge Base connection\n","kb = FlotorchVectorStore(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    vectorstore_id=knowledge_base_repo\n",")\n","\n","# 3. Load prompts into variables\n","system_prompt = prompt_config.get(\"system_prompt\", \"\")\n","user_prompt_template = prompt_config.get(\"user_prompt\", \"{question}\")\n","\n","print(\"Models and Knowledge Base are ready.\")"]},{"cell_type":"markdown","metadata":{"id":"f51660d1"},"source":["## 7. Run Experiment Loop\n","\n","### Purpose\n","Execute the full RAG pipeline for each question to generate answers for evaluation.\n","\n","### Pipeline Steps\n","For each question in `ground_truth`, the loop performs:\n","\n","1. **Retrieve Context**: Searches the Knowledge Base (`kb.search()`) to fetch relevant context passages\n","2. **Build Messages**: Uses `create_messages()` to format the system prompt, user prompt, question, and retrieved context into LLM-ready messages\n","3. **Generate Answer**: Invokes the inference LLM (`inference_llm.invoke()`) with `return_headers=True` to capture response metadata (cost, latency, tokens)\n","4. **Store for Evaluation**: Packages question, generated answer, expected answer, context, and metadata into an `EvaluationItem` object\n","\n","### Error Handling\n","A `try...except` block gracefully handles API failures, storing error messages as evaluation items to ensure the loop completes without crashes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0e37ac2"},"outputs":[],"source":["evaluation_items = [] # This will store our results\n","\n","# Use simple tqdm for a progress bar\n","print(f\"Running experiment on {len(ground_truth)} items...\")\n","\n","for qa in tqdm(ground_truth):\n","    question = qa.get(\"question\", \"\")\n","    gt_answer = qa.get(\"answer\", \"\")\n","\n","    try:\n","        # --- 1. Retrieve Context ---\n","        search_results = kb.search(query=question)\n","        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n","\n","        # --- 2. Build Messages ---\n","        messages = create_messages(\n","            system_prompt=system_prompt,\n","            user_prompt_template=user_prompt_template,\n","            question=question,\n","            context=context_texts\n","        )\n","\n","        # --- 3. Generate Answer ---\n","        # return_headers=True captures gateway metrics (LATENCY, COST, TOKEN_USAGE) automatically\n","        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n","        generated_answer = response.content\n","\n","        # --- 4. Store for Evaluation ---\n","        # Headers contain gateway metrics automatically tracked by Flotorch Gateway:\n","        # - LATENCY: total and average latency (ms)\n","        # - COST: total cost of operations (USD)\n","        # - TOKEN_USAGE: total tokens consumed (input + output)\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=generated_answer,\n","            expected_answer=gt_answer,\n","            context=context_texts, # Store the context for later display\n","            metadata=headers, # Gateway metrics extracted from headers automatically\n","        ))\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n","        # Store a failure case so we can see it\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=f\"Error: {e}\",\n","            expected_answer=gt_answer,\n","            context=[],\n","            metadata={\"error\": str(e)},\n","        ))\n","\n","print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"]},{"cell_type":"markdown","metadata":{"id":"31UKhdQqw0c5"},"source":["\n"]},{"cell_type":"markdown","metadata":{"id":"crt03A2aphia"},"source":["## 8. Initialize the Evaluator\n","\n","### Using Ragas Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can set up the `LLMEvaluator`.\n","\n","This class is the core component of the **Flotorch-Eval** library — think of it as the *“head judge”* for our evaluation process. It coordinates metric calculations, semantic comparisons, and LLM-based judgments using the configuration we provide.\n","\n","### Parameter Insights\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — The evaluator uses:\n","  - an **LLM** (`inferencer_model`) for reasoning-based checks, and  \n","  - an **embedding model** (`embedding_model`) for semantic and contextual similarity evaluations.  \n","- **`evaluation_engine`** — Here, we set this to `\"ragas\"`, meaning the evaluator will use the **[Ragas framework](https://docs.ragas.io/en/stable/getstarted/)** for metric computation.  \n","  Ragas is well-suited for RAG-style evaluations and handles metrics such as:\n","  - **Faithfulness**\n","  - **Answer Relevance**\n","  - **Context Precision**\n","  - **Aspect Critic (custom maliciousness check)**  \n","- **`metrics`** — In this configuration, we evaluate only **`MetricKey.FAITHFULNESS`**.\n","\n","### Gateway Metrics - Automatic Extraction\n","\n","**Important**: Gateway metrics (LATENCY, COST, TOKEN_USAGE) are **automatically extracted** from the `metadata` field (response headers) of each `EvaluationItem`.\n","\n","When you pass headers from `FlotorchLLM.invoke(return_headers=True)` as metadata, the evaluator automatically:\n","- Extracts **LATENCY** (total_latency_ms, average_latency_ms) from gateway response headers\n","- Calculates **COST** (total_cost) from gateway pricing data\n","- Aggregates **TOKEN_USAGE** (total_tokens) from gateway headers\n","\n","**No additional configuration needed** - gateway metrics are collected transparently! See the [Flotorch Eval documentation](https://github.com/FissionAI/flotorch-eval/tree/develop) for details.\n","\n","### Faithfulness Metric\n","\n","**Definition**: evaluates how factually consistent a generated response is with the retrieved context. It measures whether all claims in the generated answer can be supported by the context. The score ranges from 0 to 1, calculated as: **Number of claims supported by context / Total number of claims in the response**. This metric is crucial for preventing hallucinations and ensuring the AI doesn't fabricate information beyond what's provided in the source documents.\n","\n","**How It Works**:\n","1. Breaks the answer into individual claims\n","2. Checks each claim against retrieved context  \n","3. Score = (Supported claims) / (Total claims)\n","\n","**Example**:\n","\n","*Context*: \"Agriculture employs approximately 26 percent of the global workforce and contributes significantly to the gross domestic product of many nations, particularly in developing countries. The three main types of irrigation methods are surface irrigation, sprinkler irrigation, and drip irrigation.\"\n","\n","*Faithful Answer* (Correct): \"Agriculture employs approximately 26 percent of the global workforce. The three main irrigation methods are surface, sprinkler, and drip irrigation.\" → **Score: 1.0**\n","\n","*Unfaithful Answer* (Incorrect): \"Agriculture employs 50 percent of the global workforce, and there are five main types of irrigation methods.\" → **Score: 0.0** (contains unsupported claims)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5e9f8a1"},"outputs":[],"source":["# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.FAITHFULNESS,\n","    ],\n","    evaluation_engine=\"ragas\",\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"a4d3b6f0"},"source":["## 9. Run Evaluation\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **faithfulness** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **faithfulness** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Faithfulness scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute faithfulness scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","source":["### Asynchronous Evaluation"],"metadata":{"id":"zi1zDTska31n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1d0e3b2"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"],"metadata":{"id":"575QNWj2bBWZ"}},{"cell_type":"code","source":["# print(\"Starting evaluation... This may take a few minutes.\")\n","\n","# eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","# print(\"Evaluation complete.\")"],"metadata":{"id":"Jsqmq5mCa819"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e8f1c3a7"},"source":["## 10. View Per-Question Results\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9g2h4j6"},"outputs":[],"source":["display_llm_evaluation_results(eval_result)"]},{"cell_type":"markdown","metadata":{"id":"k8l9m0n1"},"source":["## 11. View Raw JSON Results\n","\n","### Purpose\n","Display the complete evaluation results in JSON format for detailed inspection and programmatic access.\n","\n","### Output Structure\n","The JSON output includes for each question:\n","- **model**: The evaluation LLM model used\n","- **input_query**: The original question\n","- **context**: Full retrieved context passages (not truncated)\n","- **generated_answer**: Complete LLM-generated response\n","- **groundtruth_answer**: Expected correct answer\n","- **evaluation_metrics**: Dictionary containing:\n","\n","  **Gateway Metrics (Automatically Tracked):**\n","  - **total_latency_ms**: Total latency across all LLM operations (milliseconds)\n","  - **average_latency_ms**: Average latency per operation (milliseconds)\n","  - **total_cost**: Total cost of all LLM operations (USD)\n","  - **total_tokens**: Total token consumption (input + output tokens)\n","\n","  **Quality Metrics:**\n","  - **faithfulness**: Faithfulness score (0.0 to 1.0)\n","  - **average_score**: Average of all evaluated metrics\n","  - **total_latency_ms**: Total latency across all LLM operations (milliseconds)\n","  - **average_latency_ms**: Average latency per operation (milliseconds)\n","  - **total_cost**: Total cost of all LLM operations (USD)\n","  - **total_tokens**: Total token consumption (input + output tokens)\n","\n","**Gateway Metrics Details:**\n","\n","These gateway metrics are automatically extracted from the Flotorch Gateway response headers. They provide operational insights:\n","- **LATENCY**: Track response times to optimize performance\n","- **COST**: Monitor spending across all model calls\n","- **TOKEN_USAGE**: Understand consumption patterns for capacity planning\n","\n","All gateway metrics are collected transparently - simply pass headers as metadata and the evaluator extracts them automatically.\n","\n","This raw JSON format is useful for further analysis, exporting results, cost tracking, performance monitoring, or integrating with other tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2q3r4s5"},"outputs":[],"source":["print(\"--- Aggregate Evaluation Results ---\")\n","print(json.dumps(eval_results, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"summary_cell_new"},"source":["## 12. Summary\n","\n","### What We Accomplished\n","\n","This notebook provided a complete, step-by-step workflow for tracking and evaluating gateway metrics (latency, cost, token usage) using Flotorch Eval.\n","\n","### Workflow Summary\n","\n","1. **Configured Infrastructure**\n","   - Set up `FlotorchLLM` for answer generation\n","   - Connected to `FlotorchVectorStore` for context retrieval\n","   - Initialized `LLMEvaluator` with Ragas engine for faithfulness scoring\n","\n","2. **Generated Responses**\n","   - Loaded ground truth questions from `gt.json`\n","   - Retrieved relevant context from the Knowledge Base for each question\n","   - Generated answers using the inference LLM with retrieved context\n","   - **Captured gateway metrics** by using `return_headers=True` with `FlotorchLLM.invoke()`\n","   - **Automatically collected LATENCY, COST, and TOKEN_USAGE** from gateway response headers\n","\n","3. **Evaluated Gateway Metrics & Quality Metrics**\n","  - **Automatically extracted gateway metrics** (LATENCY, COST, TOKEN_USAGE) from response headers\n","  - Scored each generated answer using the Ragas faithfulness metric\n","  - Combined operational metrics (latency, cost, tokens) with quality metrics (faithfulness) for comprehensive analysis\n","\n","4. **Visualized Results**\n","  - Displayed **gateway metrics** (latency, cost, tokens) in a formatted table for quick analysis\n","  - Exported complete results as JSON including **all gateway metrics** (latency, cost, tokens)\n","  - Analyzed operational metrics (performance, cost, usage) alongside quality metrics\n","  - Monitored gateway metrics for cost and performance optimization\n","\n","### Key Takeaways\n","\n","#### Gateway Metrics (Automatically Tracked)\n","- **LATENCY**: Automatically tracked from Flotorch Gateway response headers - measures total and average latency across all LLM calls\n","- **COST**: Automatically extracted from gateway pricing data - tracks total cost of all LLM operations in USD\n","- **TOKEN_USAGE**: Automatically aggregated from gateway headers - monitors total token consumption (input + output)\n","\n","**Gateway Metrics Benefits:**\n","- **No additional configuration required** - simply use `return_headers=True` with `FlotorchLLM.invoke()`\n","- **Transparent collection** - metrics are automatically extracted from response headers\n","- **Comprehensive tracking** - every LLM operation is automatically monitored\n","- **Cost optimization** - track spending patterns to optimize model usage\n","- **Performance monitoring** - identify latency bottlenecks for faster responses\n","- **Usage analytics** - understand token consumption for capacity planning\n","\n","For detailed gateway metrics documentation, see the [Flotorch Eval GitHub repository](https://github.com/FissionAI/flotorch-eval/tree/develop).\n","\n","**Example Context**: “Corn requires 150-200 pounds of nitrogen per acre for optimal yields. Apply nitrogen in split applications: 50% at planting, 25% at vegetative stage, 25% at reproductive stage.”\n","\n","- **Ragas Outcome**\n","  - Faithful answer: “Corn needs 150-200 pounds of nitrogen per acre, split into three applications at planting, vegetative, and reproductive stages” → score ≈ 1.0\n","  - Unfaithful answer: “Corn requires 300 pounds of nitrogen applied all at once at planting” → unsupported claim lowers score toward 0\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venve","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":0}