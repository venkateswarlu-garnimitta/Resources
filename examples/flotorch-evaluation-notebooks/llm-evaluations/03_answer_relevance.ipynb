{"cells":[{"cell_type":"markdown","metadata":{"id":"g0RWm8NGakYg"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1Y2sT9fbvpn21KfKG-9sDBelT0SE3_ArU/view?usp=sharing)\n","\n","# Evaluating the Medical Assistant with Flotorch Eval"]},{"cell_type":"markdown","metadata":{"id":"jJLsbVh3ER1Y"},"source":["This notebook walks through evaluating a **Medical Assistant**—a retrieval-augmented agent that answers questions about clinical conditions, treatments, and patient education—using the **Flotorch SDK** together with the **Flotorch Eval** library.\n","\n","---\n","\n","### **Use Case Overview**\n","\n","The assistant guides clinicians and patients across:\n","- **Cardiovascular care** — hypertension workups, coronary artery disease management, heart failure escalation plans\n","- **Respiratory medicine** — asthma action plans, COPD therapy ladders, pneumonia treatment guidelines\n","- **Digestive health** — gastrointestinal differential diagnoses, nutrition considerations, post-operative monitoring\n","- **Endocrine and metabolic disorders** — diabetes protocols, thyroid dysfunction pathways, hormone replacement nuances\n","- **Neurology and behavioral health** — migraine regimens, neurodegenerative care plans, mental health screenings\n","\n","It retrieves evidence-backed content from a **Comprehensive Medical Reference Guide** (diagnostic criteria, treatment pathways, medication guidance, patient handouts) so that generated advice remains grounded in current clinical practice.\n","\n","This notebook evaluates the **answer relevance** of those responses—confirming that each answer stays on-topic and directly addresses the medical question.\n","\n","---\n","\n","### **Notebook Workflow**\n","\n","We follow a structured evaluation loop:\n","\n","1. **Iterate Questions** – Read each prompt from `medical_eval.json` (ground truth set).  \n","2. **Retrieve Context** – Query the medical knowledge base for the most relevant passages.  \n","3. **Generate Answer** – Compose a response with the shared system prompt and inference LLM.  \n","4. **Store Results** – Record question, context, generated answer, and expected answer.  \n","5. **Evaluate Answer Relevance** – Score topical alignment with `LLMEvaluator`.  \n","6. **Display Results** – Compare Ragas and DeepEval outputs side by side.\n","\n","---\n","\n","### **Metric Evaluated — Answer Relevance**\n","\n","We focus on a single answer-quality signal: **Answer Relevance**. It asks whether the response directly addresses the user’s question, independent of factual accuracy. Scores near 1.0 mean the assistant stayed on topic, mid-range scores expose partial coverage, and low scores flag answers that ignored the request—giving us an immediate cue for debugging prompts, retrieval, or guardrails.\n","\n","#### Ragas Answer Relevance (Flotorch `evaluation_engine=\"ragas\"`)\n","- Generates auxiliary questions from the model’s answer using an LLM.  \n","- Embeds those questions and the original prompt to measure similarity.  \n","- Reports an average similarity score (0.0–1.0) that reflects topical alignment.\n","\n","#### DeepEval Answer Relevance (Flotorch `evaluation_engine=\"deepeval\"`)\n","- Extracts intent-bearing snippets from the answer with DeepEval’s judging LLM.  \n","- Labels each snippet as relevant or irrelevant to the user question.  \n","- Applies optional thresholds (e.g., `0.8`) to trigger pass/fail outcomes and surfaces rationales.\n","\n","Both engines share the same objective—“does the answer actually address the question?”—but emphasize different diagnostics: Ragas provides fast, retrieval-aware scoring; DeepEval adds snippet-level verdicts and guardrail-friendly thresholds.\n","\n","---\n","\n","### **Evaluation Engine**\n","\n","- `evaluation_engine=\"auto\"` — lets Flotorch Eval mix Ragas and DeepEval according to the priority routing described in the [flotorch-eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop) so that retrieval metrics default to the best available backend.\n","- `evaluation_engine=\"ragas\"` — keeps every metric inside the [**Ragas**](https://docs.ragas.io/en/stable/getstarted/) rubric for retrieval-aware evaluations (faithfulness, answer relevance, context precision, aspect critic, etc.).\n","- `evaluation_engine=\"deepeval\"` — routes metrics through DeepEval’s judging engine, enabling custom prompts, thresholds, and rationales while still capturing Flotorch gateway telemetry.\n","\n","We begin with the Ragas-only flow and then repeat the run with the DeepEval backend to highlight differences in scoring and diagnostics.\n","\n","---\n","\n","### **Requirements**\n","\n","- Flotorch account with configured LLM, embedding model, and Knowledge Base.  \n","- `gt.json` containing question–answer pairs for evaluation.  \n","- `prompt.json` containing the system and user prompt templates.\n","\n","---\n","#### **Documentation References**\n","\n","- [**flotorch-eval GitHub repo**](https://github.com/FissionAI/flotorch-eval/tree/develop) for reference pipelines.  \n","- [**Ragas Answer Relevance**](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) - detailed explanation of the metric.\n","- [**DeepEval Answer Relevance**](https://deepeval.com/docs/metrics-answer-relevancy) - detailed explanation of the metric.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"SVO_YeJSER1i"},"source":[]},{"cell_type":"markdown","metadata":{"id":"e2a09f3e"},"source":["## 1. Install Dependencies\n","\n","First, we install the two necessary libraries. The `-q` flag is for a \"quiet\" installation, hiding the lengthy output.\n","\n","-   `flotorch`: The main Python SDK for interacting with all Flotorch services, including LLMs and Knowledge Bases.\n","-   `flotorch-eval[llm]`: The evaluation library. We use the `[llm]` extra to install dependencies required for LLM-based (Ragas) evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwnpCfiUXfJ_"},"outputs":[],"source":["# Install flotorch-sdk and flotorch-core\n","# You can safely ignore dependency errors during installation.\n","\n","%pip install flotorch==3.1.0b1 flotorch-eval==2.0.0\n","%pip install opentelemetry-instrumentation-httpx==0.58b0"]},{"cell_type":"markdown","metadata":{"id":"e4418652"},"source":["## 2. Configure Environment\n","\n","This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n","\n","-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n","-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n","-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n","-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n","-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n","-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent.\n","\n","### Example :\n","\n","| Parameter | Description | Example |\n","|-----------|-------------|---------|\n","| `FLOTORCH_API_KEY` | Your API authentication key | `sk_...` |\n","| `FLOTORCH_BASE_URL` | Gateway endpoint | `https://gateway.flotorch.cloud` |\n","| `inference_model_name` | The LLM your agent uses to generate answers | `flotorch/gpt-4o-mini` |\n","| `evaluation_llm_model_name` | The LLM used to evaluate the answers | `flotorch/gpt-4o` |\n","| `evaluation_embedding_model_name` | Embedding model for semantic similarity checks | `open-ai/text-embedding-ada-002` |\n","| `knowledge_base_repo` | The ID of your Flotorch Knowledge Base | `digital-twin` |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkEYxQ9yRmtQ"},"outputs":[],"source":["import getpass  # Securely prompt without echoing in Prefect/notebooks\n","\n","# Prefect-side authentication for Flotorch access\n","try:\n","    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  # Used by Prefect flow and local runs\n","    print(f\"Success\")\n","except getpass.GetPassWarning as e:\n","    print(f\"Warning: {e}\")\n","    FLOTORCH_API_KEY = \"\"\n","\n","FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint\n","\n","inference_model_name = \"flotorch/<your-model-name>\"  # Model generating answers\n","evaluation_llm_model_name = \"flotorch/<your_model_name>\"  # Model judging answer quality\n","evaluation_embedding_model_name = \"flotorch/<embedding_model_name>\"  # Embedding model for similarity checks\n","\n","knowledge_base_repo = \"<your_knowledge_base_id>\" #Knowledge_base ID"]},{"cell_type":"markdown","metadata":{"id":"76978434"},"source":["## 3. Import Required Libraries\n","\n","### Purpose\n","Import all required components for evaluating the RAG assistant.\n","\n","### Key Components\n","- `json` : Loads configuration files and ground truth data from disk\n","- `tqdm` : Shows a lightweight progress bar while iterating over evaluation items\n","- `FlotorchLLM` : Connects to the Flotorch inference endpoint for answer generation\n","- `FlotorchVectorStore` : Retrieves context snippets from the configured knowledge base\n","- `memory_utils` : Utility helpers for extracting text from vector-store search results\n","- `LLMEvaluator`, EvaluationItem, MetricKey** : Runs metric scoring for the generated answers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RRnnoQfXbOM"},"outputs":[],"source":["#Required imports\n","import json\n","from typing import List\n","from tqdm import tqdm # Use standard tqdm for simple progress bars\n","from google.colab import files\n","\n","# Flotorch SDK components\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.memory import FlotorchVectorStore\n","from flotorch.sdk.utils import memory_utils\n","\n","# Flotorch Eval components\n","from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n","from flotorch_eval.llm_eval import display_llm_evaluation_results\n","\n","\n","print(\"Imported necessary libraries successfully\")"]},{"cell_type":"markdown","metadata":{"id":"8E6DGAqY7nPS"},"source":["## 4. Load Data and Prompts\n","\n","### Purpose\n","Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local\n","files.\n","\n","### Files Required\n","\n","**1. `gt.json` (Ground Truth)**  \n","Contains question-answer pairs for evaluation. Each `answer` is the expected correct response.\n","\n","```json\n","[\n","  {\n","    \"question\": \"What is the definition of hypertension according to the guide?\",\n","    \"answer\": \"Hypertension is defined as persistent elevation of blood pressure above 140/90 mmHg.\"\n","  },\n","  {\n","    \"question\": \"What is the target blood pressure for patients with chronic kidney disease?\",\n","    \"answer\": \"The target blood pressure for CKD patients is less than 130/80 mmHg.\"\n","  }\n","]\n","```\n","\n","**2. `prompt.json` (Agent Prompts)**  \n","Defines the system prompt and user prompt template with `{context}` and `{question}` placeholders for dynamic formatting.\n","\n","```json\n","{\n","  \"system_prompt\": \"You are a helpful Medical assistant. Answer based only on the context provided.\",\n","  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n","}\n","```\n","\n","### Instructions\n","Update `gt_path` and `prompt_path` variables in the next cell to point to your local file locations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFYSNDJo1MAP"},"outputs":[],"source":["print(\"Please upload your Ground Truth file (gt.json)\")\n","gt_upload = files.upload()\n","\n","gt_path = list(gt_upload.keys())[0]\n","with open(gt_path, 'r') as f:\n","    ground_truth = json.load(f)\n","print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n","\n","\n","print(\"Please upload your Prompts file (prompts.json)\")\n","prompts_upload = files.upload()\n","\n","prompts_path = list(prompts_upload.keys())[0]\n","with open(prompts_path, 'r') as f:\n","    prompt_config = json.load(f)\n","print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"]},{"cell_type":"markdown","metadata":{"id":"1d4b680f"},"source":["## 5. Define Helper Function\n","\n","### Purpose\n","Create a prompt-formatting helper for LLM message construction.\n","\n","### Functionality\n","The `create_messages` function:\n","- Builds the final prompt that will be sent to the LLM.\n","- Accepts system prompt, user prompt template, question, and retrieved context chunks\n","- Replaces `{context}` and `{question}` placeholders in the user prompt\n","- Returns a structured message list with (`{role: ..., content: ...}`) fields ready for LLM consumption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4hMY5Ucdo4"},"outputs":[],"source":["def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n","    \"\"\"\n","    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n","    \"\"\"\n","    context_text = \"\"\n","    if context:\n","        if isinstance(context, list):\n","            context_text = \"\\n\\n---\\n\\n\".join(context)\n","        elif isinstance(context, str):\n","            context_text = context\n","\n","    # Format the user prompt template\n","    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_content}\n","    ]\n","    return messages\n"]},{"cell_type":"markdown","metadata":{"id":"76e336e1"},"source":["## 6. Initialize Clients\n","\n","### Purpose\n","Set up the infrastructure for RAG pipeline execution.\n","\n","### Components Initialized\n","1. **FlotorchLLM** (`inference_llm`): Connects to the LLM endpoint for generating answers based on retrieved context\n","2. **FlotorchVectorStore** (`kb`): Connects to the Knowledge Base for semantic search and context retrieval\n","3. **Prompt Variables**: Extracts system prompt and user prompt template from `prompt_config` for dynamic message formatting\n","\n","These clients power the evaluation loop by retrieving relevant context and generating answers for each question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66f8510"},"outputs":[],"source":["# 1. Set up the LLM for generating answers\n","inference_llm = FlotorchLLM(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    model_id=inference_model_name\n",")\n","\n","# 2. Set up the Knowledge Base connection\n","kb = FlotorchVectorStore(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    vectorstore_id=knowledge_base_repo\n",")\n","\n","# 3. Load prompts into variables\n","system_prompt = prompt_config.get(\"system_prompt\", \"\")\n","user_prompt_template = prompt_config.get(\"user_prompt\", \"{question}\")\n","\n","print(\"Models and Knowledge Base are ready.\")"]},{"cell_type":"markdown","metadata":{"id":"f51660d1"},"source":["## 7. Run Experiment Loop\n","\n","### Purpose\n","Execute the full RAG pipeline for each question to generate answers for evaluation.\n","\n","### Pipeline Steps\n","For each question in `ground_truth`, the loop performs:\n","\n","1. **Retrieve Context**: Searches the Knowledge Base (`kb.search()`) to fetch relevant context passages\n","2. **Build Messages**: Uses `create_messages()` to format the system prompt, user prompt, question, and retrieved context into LLM-ready messages\n","3. **Generate Answer**: Invokes the inference LLM (`inference_llm.invoke()`) with `return_headers=True` to capture response metadata (cost, latency, tokens)\n","4. **Store for Evaluation**: Packages question, generated answer, expected answer, context, and metadata into an `EvaluationItem` object\n","\n","### Error Handling\n","A `try...except` block gracefully handles API failures, storing error messages as evaluation items to ensure the loop completes without crashes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0e37ac2"},"outputs":[],"source":["evaluation_items = [] # This will store our results\n","\n","# Use simple tqdm for a progress bar\n","print(f\"Running experiment on {len(ground_truth)} items...\")\n","\n","for qa in tqdm(ground_truth):\n","    question = qa.get(\"question\", \"\")\n","    gt_answer = qa.get(\"answer\", \"\")\n","\n","    try:\n","        # --- 1. Retrieve Context ---\n","        search_results = kb.search(query=question)\n","        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n","\n","        # --- 2. Build Messages ---\n","        messages = create_messages(\n","            system_prompt=system_prompt,\n","            user_prompt_template=user_prompt_template,\n","            question=question,\n","            context=context_texts\n","        )\n","\n","        # --- 3. Generate Answer ---\n","        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n","        generated_answer = response.content\n","\n","        # --- 4. Store for Evaluation ---\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=generated_answer,\n","            expected_answer=gt_answer,\n","            context=context_texts, # Store the context for later display\n","            metadata=headers,\n","        ))\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n","        # Store a failure case so we can see it\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=f\"Error: {e}\",\n","            expected_answer=gt_answer,\n","            context=[],\n","            metadata={\"error\": str(e)},\n","        ))\n","\n","print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"]},{"cell_type":"markdown","metadata":{"id":"crt03A2aphia"},"source":["## 8. Initialize the Evaluator\n","\n","### Using Ragas Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can set up the `LLMEvaluator`.\n","\n","This class is the core component of the **Flotorch-Eval** library — think of it as the *“head judge”* for our evaluation process. It coordinates metric calculations, semantic comparisons, and LLM-based judgments using the configuration we provide.\n","\n","### Parameter Insights\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — The evaluator uses:\n","  - an **LLM** (`inferencer_model`) for reasoning-based checks, and  \n","  - an **embedding model** (`embedding_model`) for semantic and contextual similarity evaluations.  \n","- **`evaluation_engine`** — Here, we set this to `\"ragas\"`, meaning the evaluator will use the **[Ragas framework](https://docs.ragas.io/en/stable/getstarted/)** for metric computation.  \n","  Ragas is well-suited for RAG-style evaluations and handles metrics such as:\n","  - **Faithfulness**\n","  - **Answer Relevance**\n","  - **Context Precision**\n","  - **Aspect Critic (custom maliciousness check)**  \n","- **`metrics`** — In this configuration, we evaluate only **`MetricKey.ANSWER_RELEVANCE`**.\n","\n","### Answer Relevance Metric\n","\n","**Definition**: evaluates how pertinent and directly relevant the generated answer is to the given question. It measures whether the response actually addresses what was asked, regardless of factual accuracy. The score ranges from 0 to 1, calculated by: **generating questions from the answer using an LLM, then computing semantic similarity between generated questions and the original question**. This metric is crucial for ensuring the Medical Assistant provides on-topic, focused responses that directly address user queries about medical conditions.\n","\n","**How It Works**:\n","1. Generate multiple questions from the given answer using an LLM\n","2. Compute semantic similarity between generated questions and the original question using embeddings\n","3. Score = Average similarity (higher similarity = more relevant answer)\n","\n","**Example**:\n","\n","*Question*: \"What are the common symptoms of Type 2 Diabetes?\"\n","\n","*Relevant Answer* (High Relevance): \"Common symptoms of Type 2 Diabetes include increased thirst, frequent urination, increased hunger, fatigue, blurred vision, and slow-healing sores.\" → **Score: 1.0**\n","\n","*Irrelevant Answer* (Low Relevance): \"Hypertension is treated with ACE inhibitors and beta-blockers. Patients should monitor blood pressure regularly.\" → **Score: 0.0** (talks about hypertension instead of diabetes symptoms)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5e9f8a1"},"outputs":[],"source":["# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.ANSWER_RELEVANCE,\n","    ],\n","    evaluation_engine=\"ragas\"\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"a4d3b6f0"},"source":["## 9. Run Evaluation (Ragas)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **answer relevance** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **answer relevance** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Answer relevance scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute answer relevance scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"7UPySX4fSBZE"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1d0e3b2"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"Y2ebXKYoSBZE"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1I8e9fcrSBZF"},"outputs":[],"source":["# print(\"Starting evaluation... This may take a few minutes.\")\n","\n","# eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","# print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"csj4aTQhER2A"},"source":["## 10. View Per-Question Results (Ragas)\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uRvLEzbdER2A"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"xj8sHAHFER2B"},"source":["## 11. Initialize the Evaluator (DeepEval)\n","\n","### Using DeepEval Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can switch the `LLMEvaluator` to the **DeepEval** backend to compare its scoring with the Ragas run.\n","\n","This class is still the *“head judge”* for our evaluation process; we’re simply changing which judging rubric it follows.\n","\n","### Parameter Insights (DeepEval Mode)\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — DeepEval-powered scoring still needs the evaluator LLM and embeddings for semantic checks.  \n","- **`evaluation_engine=\"deepeval\"`** — Routes metrics through DeepEval, which (per the [flotorch-eval repository](https://github.com/FissionAI/flotorch-eval/tree/develop)) unlocks the following metric keys:  \n","  - **`MetricKey.FAITHFULNESS`**\n","  - **`MetricKey.ANSWER_RELEVANCY`**\n","  - **`MetricKey.CONTEXT_RELEVANCY`**\n","  - **`MetricKey.CONTEXT_PRECISION`**\n","  - **`MetricKey.CONTEXT_RECALL`**\n","  - **`MetricKey.HALLUCINATION`**\n","  These are the same metrics surfaced in Flotorch’s *auto* mode when Ragas prerequisites (like embeddings) are missing.  \n","- **`metric_configs`** — Lets us pass DeepEval-specific arguments, such as a custom threshold for answer relevancy (we set `0.8` below) plus tuning knobs for other diagnostics.\n","\n","### DeepEval Answer Relevance Metric\n","\n","**Definition**: checks whether every statement in the generated answer directly addresses the user’s question using the [DeepEval answer relevance rubric](https://deepeval.com/docs/metrics-answer-relevance). The metric returns a score between 0 and 1, where higher values indicate fully on-topic answers that satisfy the configured threshold.\n","\n","**How It Works**:\n","1. Extracts intent-carrying snippets from the answer with DeepEval’s grading LLM.\n","2. Judges each snippet against the original question, marking it relevant or irrelevant and providing short rationales.\n","3. Computes the answer relevance score as the proportion of relevant snippets, then compares the result to any threshold set in `metric_configs` (e.g., `0.8`) to drive pass/fail signals.\n","\n","**Example**:\n","\n","*Question*: \"What are the common symptoms of Type 2 Diabetes?\"\n","\n","- *Pass Scenario* (Score ≈ 1.0): The answer lists classic symptoms—polydipsia, polyuria, fatigue—so every snippet is marked relevant and the threshold is cleared.\n","- *Fail Scenario* (Score ≈ 0.0): The answer pivots to hypertension treatments; DeepEval flags each snippet as irrelevant and explains the mismatch.\n","\n","This mirrors the Ragas workflow while adding threshold-driven outcomes and explanatory diagnostics useful for automated guardrails.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBOM44cTER2C"},"outputs":[],"source":["# Configure DeepEval-specific metric arguments\n","metric_args = {\n","    \"answer_relevancy\": {\"threshold\": 0.7},\n","}\n","\n","# Initialize the LLMEvaluator client using DeepEval\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.ANSWER_RELEVANCE,\n","    ],\n","    evaluation_engine=\"deepeval\",\n","    metric_configs=metric_args,\n",")\n","\n","print(\"LLMEvaluator client initialized.\")\n"]},{"cell_type":"markdown","metadata":{"id":"YTPPjLU-ER2C"},"source":["## 12. Run Evaluation (DeepEval)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **answer relevance** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **answer relevance** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Answer relevance scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute answer relevance scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"VAVsoCv6SBZG"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7PROa4ZER2D"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")\n"]},{"cell_type":"markdown","metadata":{"id":"SwojXPGySBZG"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hhZ_4npJSBZG"},"outputs":[],"source":["# print(\"Starting evaluation... This may take a few minutes.\")\n","\n","# eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","# print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"Rne6nD6LER2D"},"source":["## 13. View Per-Question Results (DeepEval)\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrXN0uyMER2E"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"k8l9m0n1"},"source":["## 14. View Raw JSON Results\n","\n","### Purpose\n","Display the complete evaluation results in JSON format for detailed inspection and programmatic access.\n","\n","### Output Structure\n","The JSON output includes for each question:\n","- **model**: The evaluation LLM model used\n","- **input_query**: The original question\n","- **context**: Full retrieved context passages (not truncated)\n","- **generated_answer**: Complete LLM-generated response\n","- **groundtruth_answer**: Expected correct answer\n","- **evaluation_metrics**: Dictionary containing:\n","  - **Answer Relevance**: Answer relevance score (0.0 to 1.0)\n","  - **average_score**: Average of all evaluated metrics\n","  - **total_latency_ms**: Total evaluation time in milliseconds\n","  - **total_cost**: Cost of evaluation in USD\n","  - **total_tokens**: Token count for evaluation\n","\n","This raw JSON format is useful for further analysis, exporting results, or integrating with other tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p2q3r4s5"},"outputs":[],"source":["print(\"--- Aggregate Evaluation Results ---\")\n","print(json.dumps(eval_results, indent=2))"]},{"cell_type":"markdown","metadata":{"id":"summary_cell_new"},"source":["## 15. Summary\n","\n","### What We Accomplished\n","\n","This notebook now demonstrates a full answer-relevance evaluation workflow using **both** the Ragas and DeepEval engines. We can baseline response relevance with Ragas and then validate the same evaluation set with DeepEval’s thresholded judging rubric.\n","\n","### Workflow Summary\n","\n","1. **Configured Infrastructure**\n","   - Set up `FlotorchLLM` for answer generation.\n","   - Connected to `FlotorchVectorStore` for context retrieval.\n","   - Prepared shared prompts, credentials, and evaluation artifacts.\n","\n","2. **Generated Responses**\n","   - Loaded prompts and ground-truth questions from `medical_eval.json`.\n","   - Retrieved the top medical context passages per query.\n","   - Generated answers with the inference LLM while collecting latency, cost, and token metadata.\n","\n","3. **Scored Answer Relevance Twice**\n","   - **Ragas run**: computed answer relevance scores via semantic similarity to establish a baseline.\n","   - **DeepEval run**: re-scored the same `evaluation_items` with `evaluation_engine=\"deepeval\"`, applying a `0.8` threshold and capturing rationales plus telemetry for each verdict.\n","\n","4. **Visualized & Exported Results**\n","   - Rendered separate tables for Ragas and DeepEval outcomes to compare how each engine rates the same answers.\n","   - Provided a combined JSON export that includes metric scores and gateway telemetry for downstream analysis or automation.\n","\n","### Key Takeaways\n","\n","- **Answer Relevance ≈ 1.0** signals that the generated answer fully addresses the medical question; scores near **0.0** expose off-topic, incomplete, or hallucinated guidance.\n","- Ragas delivers fast, retrieval-aware scoring that’s easy to track across experimentation, regression suites, and tuning cycles.\n","- DeepEval layers threshold-driven guardrails and snippet-level rationales on top of the score, helping teams triage prompt drift, injection attempts, or policy violations.\n","- Monitoring both engines together provides confidence that the assistant stays on-topic while surfacing the telemetry needed for production governance and alerting.\n","\n","### Ragas vs. DeepEval in Practice\n","\n","| Dimension | Ragas Answer Relevance | DeepEval Answer Relevance |\n","|-----------|------------------------|---------------------------|\n","| **Docs** | [Metric docs](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/answer_relevance/) | [Metric docs](https://deepeval.com/docs/metrics-answer-relevance) |\n","| **Core Flow** | Generates follow-up questions from the answer and scores semantic similarity to the original question. | Uses LLM judgements to label each answer snippet as relevant/irrelevant, aggregates relevance, and applies optional thresholds. |\n","| **Engine Toggle** | `evaluation_engine=\"ragas\"` keeps scoring inside the Ragas framework with minimal configuration. | `evaluation_engine=\"deepeval\"` unlocks DeepEval metrics per the [Flotorch Eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop). |\n","| **Additional Metrics** | Answer relevance, faithfulness, context precision, aspect critic (when configured). | Answer relevancy, faithfulness, context precision/recall, hallucination, plus threshold-based outcomes and rationale traces. |\n","\n","**Example Question**: “What are the common symptoms of Type 2 Diabetes?”\n","\n","- **Ragas Outcome**  \n","  - High-scoring run: Symptoms like polydipsia, polyuria, and fatigue appear prominently, so similarity remains near 1.0.  \n","  - Low-scoring run: The answer shifts to treatment plans without listing symptoms; the score collapses toward 0.0 even if other details sound reasonable.\n","- **DeepEval Outcome**  \n","  - Pass scenario: DeepEval marks each symptom-oriented snippet as relevant, clears the 0.8 threshold, and attaches supporting rationales.  \n","  - Fail scenario: When the answer wanders into hypertension management, DeepEval flags every off-topic snippet, explains the mismatch, and surfaces telemetry for alerting.\n","\n","DeepEval’s richer diagnostic output complements the Ragas baseline, pairing quick semantic checks with guardrail-friendly reasoning so medical stakeholders can maintain trust in the assistant’s on-topic behavior."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}