{"cells":[{"cell_type":"markdown","metadata":{"id":"7MRKQmdqaBzh"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1mpYsJOxzWxPRtXrh9VpZcKJRhXFCpXSq/view?usp=sharing)\n","\n","# Evaluating the Financial Banking Assistant with Flotorch Eval"]},{"cell_type":"markdown","metadata":{"id":"664UMp9WA9bH"},"source":["This notebook walks through evaluating a **Financial Banking Assistant**—a retrieval-augmented agent that answers questions about credit cards, loans, savings products, and investment options—using the **Flotorch SDK** alongside the **Flotorch Eval** library.\n","\n","---\n","\n","### **Use Case Overview**\n","\n","The **Financial Banking Assistant** helps customers explore comprehensive information about:\n","- **Credit Cards** — premium rewards for travel, everyday cashback programs, and student-friendly starter cards\n","- **Loans** — unsecured personal lending, fixed-rate mortgage options, and flexible auto financing plans\n","- **Investments** — high-yield savings products, laddered certificate of deposit portfolios, and managed brokerage offerings\n","- **Savings Accounts** — tiered deposit accounts spanning everyday banking, premium perks, youth-focused savings, and money-market balances\n","\n","It retrieves relevant information from a **Financial Banking Knowledge Base** containing detailed product documentation with eligibility requirements, fees, APR ranges, rewards structures, and policy information to generate helpful, accurate responses for customer inquiries.\n","\n","This notebook focuses on evaluating the **context precision** of the retrieval system — that is, whether the system retrieves **relevant and useful context** from the knowledge base that actually helps answer customer questions accurately.\n","\n","---\n","\n","### **Notebook Workflow**\n","\n","We’ll follow a structured evaluation process:\n","\n","1. **Iterate Questions** – Loop through each question in the `financial_banking_gt.json` file (Ground Truth).  \n","2. **Retrieve Context** – Fetch relevant passages from the Financial Banking Knowledge Base.  \n","3. **Generate Answer** – Use the system prompt and LLM to produce a response.  \n","4. **Store Results** – Log each question, retrieved context, generated answer, and ground truth.  \n","5. **Evaluate Context Precision** – Use `LLMEvaluator` from Flotorch Eval to assess how relevant the retrieved context is to the question.  \n","6. **Display Results** – Summarize the context precision scores in a simple comparison table.\n","\n","---\n","\n","### **Metric Evaluated — Context Precision**\n","\n","We focus on a single retrieval-quality signal: **Context Precision**. It measures how effectively the retriever ranks relevant passages above irrelevant ones for a given question. A high score means the first few chunks already contain the information needed to answer; a low score indicates off-topic passages dominate the ranking.\n","\n","#### Ragas Context Precision (Flotorch `evaluation_engine=\"ragas\"`)\n","According to the [Ragas Context Precision documentation](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/?h=context), the evaluator:\n","- inspects each retrieved chunk for relevance to the user query and reference answer,\n","- computes precision@k across the ranked list with higher weight on early positions, and\n","- reports an aggregate score between 0.0 and 1.0 that rewards “relevant-first” retrieval.\n","\n","#### DeepEval Contextual Precision (Flotorch `evaluation_engine=\"deepeval\"`)\n","Per the [DeepEval contextual precision specification](https://deepeval.com/docs/metrics-contextual-precision), the evaluator:\n","- gathers an LLM judgment for every retrieved chunk given the user query,\n","- labels each chunk as relevant or irrelevant using DeepEval’s structured rubric, and\n","- returns the proportion of chunks deemed relevant, with optional thresholds for alerting or pass/fail gating.\n","\n","Both engines target the same goal—ensuring retrieval delivers helpful evidence—but they emphasise different diagnostics. Ragas centres on ranking-aware scoring, while DeepEval exposes fine-grained relevance verdicts suitable for production monitoring.\n","\n","---\n","\n","### **Evaluation Engine**\n","\n","- `evaluation_engine=\"auto\"` — lets Flotorch Eval mix Ragas and DeepEval according to the priority routing described in the [flotorch-eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop) so that retrieval metrics default to the best available backend.\n","- `evaluation_engine=\"ragas\"` — keeps every metric inside the [**Ragas**](https://docs.ragas.io/en/stable/getstarted/) rubric for retrieval-aware evaluations (faithfulness, answer relevance, context precision, aspect critic, etc.).\n","- `evaluation_engine=\"deepeval\"` — routes metrics through DeepEval’s judging engine, enabling custom prompts, thresholds, and rationales while still capturing Flotorch gateway telemetry.\n","\n","We begin with the Ragas-only flow and then repeat the run with the DeepEval backend to highlight differences in scoring and diagnostics.  \n","\n","---\n","\n","### **Requirements**\n","\n","- Flotorch account with configured LLM, embedding model, and Knowledge Base.  \n","- `gt.json` containing question–answer pairs for evaluation.  \n","- `prompt.json` containing the system and user prompt templates.  \n","\n","---\n","\n","#### **Documentation References**\n","\n","- [**flotorch-eval GitHub repo**](https://github.com/FissionAI/flotorch-eval/tree/develop) — reference implementation with sample notebooks and evaluation pipelines.\n","- [**Ragas Context Precision Documentation**](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/?h=context) — detailed explanation of the metric.\n","- [**DeepEval Contextual Precision Documentation**](https://deepeval.com/docs/metrics-contextual-precision)  — detailed explanation of the metric.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"e2a09f3e"},"source":["## 1. Install Dependencies\n","\n","First, we install the two necessary libraries. The `-q` flag is for a \"quiet\" installation, hiding the lengthy output.\n","\n","-   `flotorch`: The main Python SDK for interacting with all Flotorch services, including LLMs and Knowledge Bases.\n","-   `flotorch-eval[llm]`: The evaluation library. We use the `[llm]` extra to install dependencies required for LLM-based (Ragas) evaluations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwnpCfiUXfJ_"},"outputs":[],"source":["# Install flotorch-sdk and flotorch-core\n","# You can safely ignore dependency errors during installation.\n","\n","%pip install flotorch==3.1.0b1 flotorch-eval==2.0.0\n","%pip install opentelemetry-instrumentation-httpx==0.58b0"]},{"cell_type":"markdown","metadata":{"id":"e4418652"},"source":["## 2. Configure Environment\n","\n","This is the main configuration step. Set your API key, base URL, and the model names you want to use.\n","\n","-   **`FLOTORCH_API_KEY`**: Your Flotorch API key (found in your Flotorch Console).\n","-   **`FLOTORCH_BASE_URL`**: Your Flotorch console instance URL.\n","-   **`inference_model_name`**: The LLM your agent uses to *generate* answers (your 'agent's brain').\n","-   **`evaluation_llm_model_name`**: The LLM used to *evaluate* the answers (the 'evaluator's brain'). This is typically a powerful, separate model like `flotorch/gpt-4o` to ensure an unbiased, high-quality judgment.\n","-   **`evaluation_embedding_model_name`**: The embedding model used for semantic similarity checks during evaluation.\n","-   **`knowledge_base_repo`**: The ID of your Flotorch Knowledge Base, which acts as the 'source of truth' for your RAG agent.\n","\n","### Example :\n","\n","| Parameter | Description | Example |\n","|-----------|-------------|---------|\n","| `FLOTORCH_API_KEY` | Your API authentication key | `sk_...` |\n","| `FLOTORCH_BASE_URL` | Gateway endpoint | `https://gateway.flotorch.cloud` |\n","| `inference_model_name` | The LLM your agent uses to generate answers | `flotorch/gpt-4o-mini` |\n","| `evaluation_llm_model_name` | The LLM used to evaluate the answers | `flotorch/gpt-4o` |\n","| `evaluation_embedding_model_name` | Embedding model for semantic similarity checks | `open-ai/text-embedding-ada-002` |\n","| `knowledge_base_repo` | The ID of your Flotorch Knowledge Base | `digital-twin` |"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wkEYxQ9yRmtQ"},"outputs":[],"source":["import getpass  # Securely prompt without echoing in Prefect/notebooks\n","\n","# Prefect-side authentication for Flotorch access\n","try:\n","    FLOTORCH_API_KEY = getpass.getpass(\"Paste your API key here: \")  # Used by Prefect flow and local runs\n","    print(f\"Success\")\n","except getpass.GetPassWarning as e:\n","    print(f\"Warning: {e}\")\n","    FLOTORCH_API_KEY = \"\"\n","\n","FLOTORCH_BASE_URL = input(\"Paste your Flotorch Base URL here: \")  # Prefect gateway or cloud endpoint\n","\n","inference_model_name = \"flotorch/<your-model-name>\"  # Model generating answers\n","evaluation_llm_model_name = \"flotorch/<your_model_name>\"  # Model judging answer quality\n","evaluation_embedding_model_name = \"flotorch/<embedding_model_name>\"  # Embedding model for similarity checks\n","\n","knowledge_base_repo = \"<your_knowledge_base_id>\" #Knowledge_base ID"]},{"cell_type":"markdown","metadata":{"id":"76978434"},"source":["## 3. Import Required Libraries\n","\n","### Purpose\n","Import all required components for evaluating the RAG assistant.\n","\n","### Key Components\n","- `json` : Loads configuration files and ground truth data from disk\n","- `tqdm` : Shows a lightweight progress bar while iterating over evaluation items\n","- `FlotorchLLM` : Connects to the Flotorch inference endpoint for answer generation\n","- `FlotorchVectorStore` : Retrieves context snippets from the configured knowledge base\n","- `memory_utils` : Utility helpers for extracting text from vector-store search results\n","- `LLMEvaluator`, EvaluationItem, MetricKey** : Runs metric scoring for the generated answers\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2RRnnoQfXbOM"},"outputs":[],"source":["#Required imports\n","import json\n","from typing import List\n","from tqdm import tqdm # Use standard tqdm for simple progress bars\n","from google.colab import files\n","\n","# Flotorch SDK components\n","from flotorch.sdk.llm import FlotorchLLM\n","from flotorch.sdk.memory import FlotorchVectorStore\n","from flotorch.sdk.utils import memory_utils\n","\n","# Flotorch Eval components\n","from flotorch_eval.llm_eval import LLMEvaluator, EvaluationItem, MetricKey\n","from flotorch_eval.llm_eval import display_llm_evaluation_results\n","\n","print(\"Imported necessary libraries successfully\")"]},{"cell_type":"markdown","metadata":{"id":"8E6DGAqY7nPS"},"source":["## 4. Load Data and Prompts\n","\n","### Purpose\n","Here, we load our ground truth questions (`gt.json`) and the agent prompts (`prompt.json`) from local\n","files.\n","\n","### Files Required\n","\n","**1. `gt.json` (Ground Truth)**  \n","Contains question-answer pairs for evaluation. Each `answer` is the expected correct response.\n","\n","```json\n","[\n","    {\n","        \"question\": \"What is the APR range for the Premium Rewards Credit Card?\",\n","        \"answer\": \"The APR range for the Premium Rewards Credit Card is 18.99% - 26.99% Variable APR based on creditworthiness.\"\n","    },\n","    {\n","        \"question\": \"What is the minimum credit score required for a Personal Loan Prime?\",\n","        \"answer\": \"The minimum credit score required for a Personal Loan Prime is 700+.\",\n","    }\n","]\n","```\n","\n","**2. `prompt.json` (Agent Prompts)**  \n","Defines the system prompt and user prompt template with `{context}` and `{question}` placeholders for dynamic formatting.\n","\n","```json\n","{\n","  \"system_prompt\": \"You are a helpful Financial Banking assistant. Answer based only on the context provided.\",\n","  \"user_prompt_template\": \"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"\n","}\n","```\n","\n","### Instructions\n","Update `gt_path` and `prompt_path` variables in the next cell to point to your local file locations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFYSNDJo1MAP"},"outputs":[],"source":["print(\"Please upload your Ground Truth file (gt.json)\")\n","gt_upload = files.upload()\n","\n","gt_path = list(gt_upload.keys())[0]\n","with open(gt_path, 'r') as f:\n","    ground_truth = json.load(f)\n","print(f\"Ground truth loaded successfully — {len(ground_truth)} items\\n\")\n","\n","\n","print(\"Please upload your Prompts file (prompts.json)\")\n","prompts_upload = files.upload()\n","\n","prompts_path = list(prompts_upload.keys())[0]\n","with open(prompts_path, 'r') as f:\n","    prompt_config = json.load(f)\n","print(f\"Prompts loaded successfully — {len(prompt_config)} prompt pairs\")"]},{"cell_type":"markdown","metadata":{"id":"1d4b680f"},"source":["## 5. Define Helper Function\n","\n","### Purpose\n","Create a prompt-formatting helper for LLM message construction.\n","\n","### Functionality\n","The `create_messages` function:\n","- Builds the final prompt that will be sent to the LLM.\n","- Accepts system prompt, user prompt template, question, and retrieved context chunks\n","- Replaces `{context}` and `{question}` placeholders in the user prompt\n","- Returns a structured message list with (`{role: ..., content: ...}`) fields ready for LLM consumption"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aC4hMY5Ucdo4"},"outputs":[],"source":["def create_messages(system_prompt: str, user_prompt_template: str, question: str, context: List[str] = None):\n","    \"\"\"\n","    Creates a list of messages for the LLM based on the provided prompts, question, and optional context.\n","    \"\"\"\n","    context_text = \"\"\n","    if context:\n","        if isinstance(context, list):\n","            context_text = \"\\n\\n---\\n\\n\".join(context)\n","        elif isinstance(context, str):\n","            context_text = context\n","\n","    # Format the user prompt template\n","    user_content = user_prompt_template.replace(\"{context}\", context_text).replace(\"{question}\", question)\n","\n","    messages = [\n","        {\"role\": \"system\", \"content\": system_prompt},\n","        {\"role\": \"user\", \"content\": user_content}\n","    ]\n","    return messages\n"]},{"cell_type":"markdown","metadata":{"id":"76e336e1"},"source":["## 6. Initialize Clients\n","\n","### Purpose\n","Set up the infrastructure for RAG pipeline execution.\n","\n","### Components Initialized\n","1. **FlotorchLLM** (`inference_llm`): Connects to the LLM endpoint for generating answers based on retrieved context\n","2. **FlotorchVectorStore** (`kb`): Connects to the Knowledge Base for semantic search and context retrieval\n","3. **Prompt Variables**: Extracts system prompt and user prompt template from `prompt_config` for dynamic message formatting\n","\n","These clients power the evaluation loop by retrieving relevant context and generating answers for each question."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a66f8510"},"outputs":[],"source":["# 1. Set up the LLM for generating answers\n","inference_llm = FlotorchLLM(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    model_id=inference_model_name\n",")\n","\n","# 2. Set up the Knowledge Base connection\n","kb = FlotorchVectorStore(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    vectorstore_id=knowledge_base_repo\n",")\n","\n","# 3. Load prompts into variables\n","system_prompt = prompt_config.get(\"system_prompt\", \"\")\n","user_prompt_template = prompt_config.get(\"user_prompt\", \"{question}\")\n","\n","print(\"Models and Knowledge Base are ready.\")"]},{"cell_type":"markdown","metadata":{"id":"f51660d1"},"source":["## 7. Run Experiment Loop\n","\n","### Purpose\n","Execute the full RAG pipeline for each question using real Knowledge Base retrieval to generate answers for evaluation.\n","\n","### Pipeline Steps\n","For each question in `ground_truth`, the loop performs:\n","\n","1. **Retrieve Context**: Searches the Knowledge Base (`kb.search()`) to fetch relevant context passages based on the question\n","2. **Build Messages**: Uses `create_messages()` to format the system prompt, user prompt, question, and retrieved context into LLM-ready messages\n","3. **Generate Answer**: Invokes the inference LLM (`inference_llm.invoke()`) with `return_headers=True` to capture response metadata (cost, latency, tokens)\n","4. **Store for Evaluation**: Packages question, generated answer, expected answer, context, and metadata into an `EvaluationItem` object\n","\n","### Error Handling\n","A `try...except` block gracefully handles API failures, storing error messages as evaluation items to ensure the loop completes without crashes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e0e37ac2"},"outputs":[],"source":["evaluation_items = [] # This will store our results\n","\n","# Use simple tqdm for a progress bar\n","print(f\"Running experiment on {len(ground_truth)} items...\")\n","\n","for qa in tqdm(ground_truth):\n","    question = qa.get(\"question\", \"\")\n","    gt_answer = qa.get(\"answer\", \"\")\n","\n","    try:\n","        # --- 1. Retrieve Context ---\n","        search_results = kb.search(query=question)\n","        context_texts = memory_utils.extract_vectorstore_texts(search_results)\n","\n","        # --- 2. Build Messages ---\n","        messages = create_messages(\n","            system_prompt=system_prompt,\n","            user_prompt_template=user_prompt_template,\n","            question=question,\n","            context=context_texts\n","        )\n","\n","        # --- 3. Generate Answer ---\n","        response, headers = inference_llm.invoke(messages=messages, return_headers=True)\n","        generated_answer = response.content\n","\n","        # --- 4. Store for Evaluation ---\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=generated_answer,\n","            expected_answer=gt_answer,\n","            context=context_texts, # Store the context for later display\n","            metadata=headers,\n","        ))\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Failed on question '{question[:50]}...': {e}\")\n","        # Store a failure case so we can see it\n","        evaluation_items.append(EvaluationItem(\n","            question=question,\n","            generated_answer=f\"Error: {e}\",\n","            expected_answer=gt_answer,\n","            context=[],\n","            metadata={\"error\": str(e)},\n","        ))\n","\n","print(f\"Experiment completed. {len(evaluation_items)} items are ready for evaluation.\")"]},{"cell_type":"markdown","metadata":{"id":"crt03A2aphia"},"source":["## 8. Initialize the Evaluator\n","\n","### Using Ragas Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can set up the `LLMEvaluator`.\n","\n","This class is the core component of the **Flotorch-Eval** library — think of it as the *“head judge”* for our evaluation process. It coordinates metric calculations, semantic comparisons, and LLM-based judgments using the configuration we provide.\n","\n","### Parameter Insights\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — The evaluator uses:\n","  - an **LLM** (`inferencer_model`) for reasoning-based checks, and  \n","  - an **embedding model** (`embedding_model`) for semantic and contextual similarity evaluations.  \n","- **`evaluation_engine`** — Here, we set this to `\"ragas\"`, meaning the evaluator will use the **[Ragas framework](https://docs.ragas.io/en/stable/getstarted/)** for metric computation.  \n","  Ragas is well-suited for RAG-style evaluations and handles metrics such as:\n","  - **Faithfulness**\n","  - **Answer Relevance**\n","  - **Context Precision**\n","  - **Aspect Critic (custom maliciousness check)**  \n","\n","- **`metrics`** — In this configuration, we evaluate only **`MetricKey.CONTEXT_PRECISION`**.\n","\n","### Context Precision Metric\n","\n","**Definition**: measures how relevant the retrieved context chunks are to the question. The score ranges from 0 to 1 and reflects **the proportion of top-ranked chunks that actually help answer the question**. High precision confirms the retriever surfaces the right evidence first; low precision reveals noisy or off-topic context.\n","\n","**How It Works**:\n","1. Inspects each retrieved chunk for relevance to the question (and reference answer when available)\n","2. Labels chunks as relevant or irrelevant and applies higher weight to earlier ranks\n","3. Computes precision over the ranking to produce the final score\n","\n","**Example**:\n","\n","*Question*: \"What is the annual fee for the Premium Rewards Credit Card?\"\n","\n","*Relevant Context* (High Score): \"Premium Rewards Credit Card - Product Code: CC-PR-001, Annual Fee: $195 (waived first year for qualified applicants), APR Range: 18.99% - 26.99%...\" → **Score: 1.0**\n","\n","*Irrelevant Context* (Low Score): \"High-Yield Savings Account offers 4.35% APY with FDIC insurance up to $250,000. Minimum opening deposit is $100...\" → **Score: 0.0**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5e9f8a1"},"outputs":[],"source":["# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.CONTEXT_PRECISION,\n","    ],\n","    evaluation_engine=\"ragas\"\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"a4d3b6f0"},"source":["## 9. Run Evaluation (Ragas)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **Context precision** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **context precision** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Context precision scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute context precision scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"naOYMPZuQXRg"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8GmItgtQXRg"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"MlxoHmBJQXRg"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1d0e3b2"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"e8f1c3a7"},"source":["## 10. View Per-Question Results (Ragas)\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SKkQ6EAvQXRg"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"YO3XKpIEA9bf"},"source":["# 11. Initialize the Evaluator\n","\n","### Using DeepEval Engine\n","\n","Now that we have our `evaluation_items` list (containing the generated answers), we can switch the `LLMEvaluator` to the **DeepEval** backend to compare its scoring with the Ragas run.\n","\n","This class is still the *“head judge”* for our evaluation process; we’re simply changing which judging rubric it follows.\n","\n","### Parameter Insights (DeepEval Mode)\n","\n","- **`api_key` / `base_url`** — Standard credentials used to authenticate and connect with the Flotorch-Eval service.  \n","- **`inferencer_model` / `embedding_model`** — DeepEval-powered scoring still needs the evaluator LLM and embeddings for semantic checks.  \n","- **`evaluation_engine=\"deepeval\"`** — Routes metrics through DeepEval, which (per the [flotorch-eval repository](https://github.com/FissionAI/flotorch-eval/tree/develop)) unlocks the following metric keys:  \n","  - **`MetricKey.FAITHFULNESS`**\n","  - **`MetricKey.ANSWER_RELEVANCY`**\n","  - **`MetricKey.CONTEXT_RELEVANCY`**\n","  - **`MetricKey.CONTEXT_PRECISION`**\n","  - **`MetricKey.CONTEXT_RECALL`**\n","  - **`MetricKey.HALLUCINATION`**\n","  These are the same metrics surfaced in Flotorch’s *auto* mode when Ragas prerequisites (like embeddings) are missing.  \n","- **`metric_configs`** — Lets us pass DeepEval-specific arguments such as custom thresholds for contextual precision (we set `0.75` below) and tuning knobs for other diagnostics (maliciousness, hallucination, etc.).\n","\n","### DeepEval Context Precision Metric\n","\n","**Definition**: checks whether each retrieved chunk is relevant to the question using the [DeepEval contextual precision rubric](https://deepeval.com/docs/metrics-contextual-precision). The metric returns a score between 0 and 1, where higher values indicate the retriever consistently surfaces helpful evidence within the top-ranked chunks.\n","\n","**How It Works**:\n","1. Extracts the ranked context list returned by the retriever.  \n","2. Labels each chunk as relevant or irrelevant given the question (and, when supplied, the reference answer).  \n","3. Computes precision over the ranking—optionally applying thresholds in `metric_configs` to drive alerts or pass/fail signalling.\n","\n","**Example**:\n","\n","*Question*: \"What is the annual fee for the Premium Rewards Credit Card?\"\n","\n","*High Precision (Score ≈ 1.0)*: Top chunks cite \"Annual Fee: $195 (waived first year for qualified applicants)\" and supporting fee details.\n","\n","*Low Precision (Score ≈ 0.0)*: Top chunks drift to unrelated savings accounts or mortgage content, so DeepEval flags the retrieval as off-topic.\n","\n","This mirrors the Ragas precision workflow while adding threshold-driven outcomes and richer chunk-by-chunk diagnostics that are useful for production monitoring.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0bZroBWFA9bf"},"outputs":[],"source":["# Configure a custom metric for context_precision\n","metric_args = {\n","\n","    \"context_precision\": {\n","        \"threshold\": 0.7\n","        }\n","\n","}\n","\n","# Initialize the LLMEvaluator client\n","evaluator_client = LLMEvaluator(\n","    api_key=FLOTORCH_API_KEY,\n","    base_url=FLOTORCH_BASE_URL,\n","    embedding_model=evaluation_embedding_model_name,\n","    inferencer_model=evaluation_llm_model_name,\n","    metrics=[\n","        MetricKey.CONTEXT_PRECISION,\n","    ],\n","    evaluation_engine=\"deepeval\",\n","    metric_configs=metric_args\n",")\n","\n","print(\"LLMEvaluator client initialized.\")"]},{"cell_type":"markdown","metadata":{"id":"yvqBk8dEA9bg"},"source":["## 12. Run Evaluation (DeepEval)\n","\n","### Purpose\n","Execute the evaluation process to score all generated answers using the **context precision** metric.\n","\n","### Process\n","- Call either:\n","  - `evaluator_client.evaluate()` for **synchronous** (sequential) execution, or  \n","  - `evaluator_client.aevaluate()` for **asynchronous** (concurrent) execution  \n","  using the complete list of `evaluation_items`.\n","\n","- For each evaluation item:\n","  - The evaluator scores **context precision** by comparing the generated answer against the retrieved context.\n","\n","- Collect the following outputs:\n","  - Context precision scores\n","  - Gateway metrics (cost, latency, token usage)\n","  - Structured evaluation results\n","\n","### Output\n","- A complete evaluation report ready for analysis.\n","\n","> **Note:**  \n","> This step may take a few minutes, as it requires LLM calls for each question to compute context precision scores.  \n","> Use the **synchronous** method for standard sequential execution, or the **asynchronous** method for faster, concurrent processing.\n"]},{"cell_type":"markdown","metadata":{"id":"4EETDCyAQXRh"},"source":["### Asynchronous Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTZd9jtoA9bg"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = await evaluator_client.aevaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"KaZVJ1ZWQXRh"},"source":["### Synchronous Evaluation (uncomment the below code to use synchronous manner)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aLSpjjDTQXRh"},"outputs":[],"source":["print(\"Starting evaluation... This may take a few minutes.\")\n","\n","eval_results = evaluator_client.evaluate(evaluation_items)\n","\n","print(\"Evaluation complete.\")"]},{"cell_type":"markdown","metadata":{"id":"H2IpfTtvA9bg"},"source":["## 13. View Per-Question Results (DeepEval)\n","\n","### Purpose\n","Display evaluation results in a formatted table for easy analysis and comparison."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D6q83ihCA9bh"},"outputs":[],"source":["display_llm_evaluation_results(eval_results)"]},{"cell_type":"markdown","metadata":{"id":"k8l9m0n1"},"source":["## 14. View Raw JSON Results\n","\n","### Purpose\n","Display the complete evaluation results in JSON format for detailed inspection, exporting, or downstream automation. The payload reflects whichever evaluation engine was executed most recently (DeepEval in this workflow).\n","\n","### Output Structure\n","Each entry contains:\n","- **model**: The evaluation LLM model that produced the scores.  \n","- **input_query**: The original user question.  \n","- **context**: Full retrieved context passages (untruncated).  \n","- **generated_answer**: Complete LLM-generated response.  \n","- **groundtruth_answer**: Expected reference answer.  \n","- **evaluation_metrics**: Dictionary with metric scores and telemetry, for example:  \n","  - **context_precision / contextual_precision**: Context precision score (0.0–1.0) under the active engine.  \n","  - **average_score**: Mean of all returned metrics.  \n","  - **total_latency_ms**: Total evaluation time in milliseconds.  \n","  - **total_cost**: Cost of the evaluation in USD.  \n","  - **total_tokens**: Token count consumed during scoring.\n","\n","Use this JSON output for further analysis, comparisons between Ragas and DeepEval runs, or integration with orchestration tools."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LeIbPp2AA9bi"},"outputs":[],"source":["import json\n","\n","print(\"--- Aggregate Evaluation Results ---\")\n","print(json.dumps(eval_results, indent=2))\n"]},{"cell_type":"markdown","metadata":{"id":"summary_cell_new"},"source":["## 15. Summary\n","\n","### What We Accomplished\n","\n","This notebook now demonstrates a full retrieval-evaluation workflow using **both** the Ragas and DeepEval engines for contextual precision. You can baseline retrieval quality with Ragas and then validate the same evaluation set with DeepEval’s thresholded judging rubric.\n","\n","### Workflow Summary\n","\n","1. **Configured Infrastructure**\n","   - Set up `FlotorchLLM` for answer generation.\n","   - Connected to `FlotorchVectorStore` for context retrieval.\n","   - Prepared shared prompts, credentials, and evaluation artifacts.\n","\n","2. **Generated Responses**\n","   - Loaded prompts and ground-truth questions from `financial_banking_gt.json`.\n","   - Retrieved top-N context chunks per query.\n","   - Generated answers with the inference LLM while collecting latency, cost, and token metadata.\n","\n","3. **Scored Retrieval Quality Twice**\n","   - **Ragas run**: computed `llm_context_precision_with_reference` to establish a retrieval baseline.\n","   - **DeepEval run**: re-scored the same `evaluation_items` with `evaluation_engine=\"deepeval\"`, applying a `0.75` contextual-precision threshold via `metric_configs` and capturing auxiliary diagnostics (answer relevancy, context recall, telemetry).\n","\n","4. **Visualized & Exported Results**\n","   - Rendered separate tables for Ragas and DeepEval outcomes to compare how each engine ranks questions.\n","   - Provided a combined JSON export that includes metric scores and gateway telemetry for downstream analysis or automation.\n","\n","### Key Takeaways\n","\n","- **Context Precision ≈ 1.0** indicates that early-ranked context chunks are highly relevant; scores near **0.0** reveal noisy retrieval.\n","- DeepEval adds thresholding plus chunk-level rationales, making it easier to trigger alerts when precision drops below operational requirements.\n","- Both engines evaluate **Retrieved Context ↔ Question relevance**, not direct answer-to-ground-truth similarity.\n","- Running both engines side-by-side offers confidence in retrieval health across experimentation, regression testing, and production monitoring.\n","\n","### Ragas vs. DeepEval in Practice\n","\n","| Dimension | Ragas Context Precision | DeepEval Context Precision |\n","|-----------|-------------------------|----------------------------|\n","| **Docs** | [Metric docs](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/context_precision/) | [Metric docs](https://deepeval.com/docs/metrics-contextual-precision) |\n","| **Core Flow** | Computes precision@k over the ranked context list, weighting early chunks and relying on embeddings for relevance checks. | Uses LLM judgements to label each chunk as relevant/irrelevant, aggregates precision, and exposes rationales plus optional thresholds. |\n","| **Engine Toggle** | `evaluation_engine=\"ragas\"` keeps scoring inside the Ragas framework with minimal configuration. | `evaluation_engine=\"deepeval\"` unlocks DeepEval metrics per the [Flotorch Eval repo](https://github.com/FissionAI/flotorch-eval/tree/develop). |\n","| **Additional Metrics** | Context precision, answer relevance, faithfulness, aspect critic (when configured). | Context precision, context recall, answer relevancy, hallucination, plus threshold-based outcomes and reasoning traces. |\n","\n","**Example Retrieval**: “What is the annual fee for the Premium Rewards Credit Card?”\n","\n","- **Ragas Outcome**\n","  - High-scoring run: Top-ranked chunks quote “Annual Fee: $195 (waived first year for qualified applicants)” so context precision stays near 1.0.\n","  - Low-scoring run: If retrieval floats savings-account details to the top, precision collapses toward 0.0 even when the downstream answer looks correct.\n","- **DeepEval Outcome**\n","  - Pass scenario: DeepEval labels the first chunks as relevant, clearing the 0.75 threshold and returning supporting rationales.\n","  - Fail scenario: When irrelevant chunks dominate, DeepEval flags the retrieval, captures explanations for each off-topic chunk, and surfaces telemetry for alerting.\n","\n","DeepEval’s broader metric set complements the Ragas baseline, giving both ranking-aware scores and guardrail-friendly diagnostics."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}