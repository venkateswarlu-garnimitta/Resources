{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# AI-Powered, Region-Aware Validation of Radiology Reports Using ACR Guidelines and Customer Rubrics"
      ],
      "metadata": {
        "id": "MdC_XRJPIdVC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0foQGC1MJwsE"
      },
      "source": [
        "\n",
        "\n",
        "Radiology reporting demands consistent, standards-aligned documentation to ensure diagnostic clarity and reduce variability across clinicians. Manually checking each report against anatomy-specific ACR(American College of Radiology) guidelines and institution/customer rubrics is accurate but time-consuming and difficult to scale.\n",
        "\n",
        "This project implements an AI-driven radiology report validator using CrewAI(https://docs.crewai.com/) orchestration with a FloTorch-backed LLM, designed as a two-stage pipeline that mirrors real-world reviewer workflow.\n",
        "\n",
        "**Workflow:**\n",
        "\n",
        "1. A lightweight classifier agent reads the report text and predicts a single anatomical region key (e.g., chest, abdomen).\n",
        "2. The system then verifies whether both an ACR guideline document and a matching customer rubric exist for that region in Google Drive (PDF/DOCX).\n",
        "3. When both standards are available, the validation agent loads them via secure tool calls (used only as internal context) and produces an adherence report with weighted scoring (ACR + rubric) and actionable improvement items.\n",
        "4. If either document is missing or fails to load, the system automatically suppresses scoring and returns a â€œNot Applicableâ€ evaluation that explains the limitationâ€”avoiding unreliable judgments and preventing guideline leakage.\n",
        "\n",
        "By combining region-aware document retrieval, strict output enforcement, and robust gateway retry handling, this workflow reduces reviewer effort, improves reporting consistency, and supports scalable quality checks for radiology documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isJarlCVFAG2"
      },
      "source": [
        "## Installation of libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Installs all required libraries for AI agent orchestration (CrewAI +\n",
        "FloTorch) and for parsing guideline documents in PDF and DOCX formats."
      ],
      "metadata": {
        "id": "QP-0o3TWH8nF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dHr-T7ZY0tX_"
      },
      "outputs": [],
      "source": [
        "# Install core orchestration + PDF + DOCX support\n",
        "\n",
        "# flotorch[crewai]:\n",
        "#   - Provides the FloTorch LLM gateway integration\n",
        "#   - Includes CrewAI-compatible LLM wrappers used by agents\n",
        "#\n",
        "# crewai:\n",
        "#   - Core multi-agent orchestration framework\n",
        "#   - Used to define Agents, Tasks, Crews, and execution flow\n",
        "#\n",
        "# crewai_tools:\n",
        "#   - Utility decorators and helpers for defining tools\n",
        "#   - Enables agents to safely call Python functions (e.g., loading guidelines)\n",
        "#\n",
        "# pdfplumber:\n",
        "#   - Extracts text from PDF documents\n",
        "#   - Used to read ACR guideline PDFs\n",
        "#\n",
        "# python-docx:\n",
        "#   - Reads and parses DOCX files\n",
        "#   - Used for customer rubrics or guideline documents stored as Word files\n",
        "#\n",
        "# -q flag:\n",
        "#   - Quiet installation to reduce notebook output noise\n",
        "\n",
        "!pip install -q flotorch[crewai] crewai crewai_tools pdfplumber python-docx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "49q0L87dPZXp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-jPhvQdFFxq"
      },
      "source": [
        "Imports core libraries, document parsers, CrewAI primitives, and configures FloTorch LLM gateway settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A0cMPPkfFDIz"
      },
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Standard library imports\n",
        "# -----------------------------\n",
        "import contextlib  # redirect stdout/stderr (silence noisy agent runs)\n",
        "import io          # in-memory text buffers for capturing logs\n",
        "import json        # parse JSON tool inputs safely\n",
        "import os          # file and directory operations\n",
        "import re          # regex for parsing/cleaning text and filenames\n",
        "import time        # sleep/backoff for transient retries\n",
        "from typing import Any, Dict, List, Optional, Tuple  # type hints\n",
        "from getpass import getpass\n",
        "\n",
        "# PDF + DOCX parsing\n",
        "import pdfplumber            # read guideline PDFs\n",
        "from docx import Document    # read rubric DOCX files\n",
        "\n",
        "\n",
        "# CrewAI primitives + tool decorator\n",
        "from crewai import Agent, Task, Crew, Process  # agent orchestration\n",
        "from crewai.tools import tool                  # expose Python tools to agents\n",
        "from flotorch.crewai.llm import FlotorchCrewAILLM"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FloTorch LLM Configuration for CrewAI"
      ],
      "metadata": {
        "id": "h1twdibTUwGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts the user for FloTorch configuration values and securely captures the API key, validating that the key is provided before proceeding."
      ],
      "metadata": {
        "id": "gxlQchqRalFy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FLOTORCH_GATEWAY_BASE_URL = input(\"Enter FloTorch Gateway URL: \").strip()\n",
        "FLOTORCH_MODEL_NAME = input(\"Enter FloTorch model name: \").strip()\n",
        "FLOTORCH_API_KEY = getpass(\"Enter FloTorch API key: \").strip()\n",
        "\n",
        "if not FLOTORCH_API_KEY:\n",
        "    raise ValueError(\"FloTorch API key cannot be empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mwlyi3lEXFb",
        "outputId": "7c50ea6f-1c96-4cfa-9cd1-d09a0dec3c93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter FloTorch Gateway URL: https://gateway.flotorch.cloud\n",
            "Enter FloTorch model name: flotorch/claude-sonnet-45\n",
            "Enter FloTorch API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model  =  FlotorchCrewAILLM(\n",
        "        model_id=FLOTORCH_MODEL_NAME,\n",
        "        api_key=FLOTORCH_API_KEY,\n",
        "        base_url=FLOTORCH_GATEWAY_BASE_URL,\n",
        "    )\n",
        "\n",
        "print(f\"Flotorch LLM model configured: {model.model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWgd9_zz1shE",
        "outputId": "c8f9164e-58d8-4158-c8ff-99eef490767f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flotorch LLM model configured: flotorch/claude-sonnet-45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive for supporting documents"
      ],
      "metadata": {
        "id": "zlvjaLDbU8ce"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5yOdKi1FMM2"
      },
      "source": [
        "Mounts Google Drive, defines guideline/rubric folders, lists supported documents, and derives available anatomical region keys from filenames.\n",
        "Please make sure you download the two folders \"rad-guidelines-acr\" and \"rad-guidelines-rubric\" and upload them into your Google Drive to make them available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3wlTiT6FPzZ",
        "outputId": "e3317d9c-5e0c-4f38-fb4e-76cac9f87466"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸ“ ACR folder: /content/drive/MyDrive/rad-guidelines-acr\n",
            "   Files: ['Chest_acr.pdf']\n",
            "   Regions: ['chest']\n",
            "\n",
            "ðŸ“ Rubric folder: /content/drive/MyDrive/rad-guidelines-rubric\n",
            "   Files: ['Chest_Radiology_Report_Rubric_Template.docx']\n",
            "   Regions: ['chest']\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access guideline and rubric files\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Folder locations\n",
        "# -----------------------------\n",
        "\n",
        "ACR_DIR = \"/content/drive/MyDrive/rad-guidelines-acr\"       # modify it with your exact location\n",
        "RUBRIC_DIR = \"/content/drive/MyDrive/rad-guidelines-rubric\"  # modify it with your exact location\n",
        "\n",
        "# Ensure folders exist (no overwrite)\n",
        "os.makedirs(ACR_DIR, exist_ok=True)\n",
        "os.makedirs(RUBRIC_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def _list_docs(folder: str) -> List[str]:\n",
        "    \"\"\"List supported PDF/DOCX files in a folder.\"\"\"\n",
        "    if not os.path.isdir(folder):\n",
        "        return []\n",
        "    return [\n",
        "        f for f in os.listdir(folder)\n",
        "        if f.lower().endswith((\".pdf\", \".docx\"))\n",
        "    ]\n",
        "\n",
        "\n",
        "def extract_region_from_filename(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Extract region key from filename using first token.\n",
        "    e.g., 'chest_acr_2022.pdf' -> 'chest'\n",
        "    \"\"\"\n",
        "    base = os.path.splitext(filename)[0].lower()\n",
        "    return re.split(r\"[_\\-\\s]+\", base)[0].strip()\n",
        "\n",
        "\n",
        "# List available documents\n",
        "ACR_FILES = _list_docs(ACR_DIR)\n",
        "RUBRIC_FILES = _list_docs(RUBRIC_DIR)\n",
        "\n",
        "# Derive unique region keys from filenames\n",
        "ACR_REGIONS = sorted({extract_region_from_filename(f) for f in ACR_FILES})\n",
        "RUBRIC_REGIONS = sorted({extract_region_from_filename(f) for f in RUBRIC_FILES})\n",
        "\n",
        "\n",
        "# Debug print: folders, files, and detected regions\n",
        "print(\"ðŸ“ ACR folder:\", ACR_DIR)\n",
        "print(\"   Files:\", ACR_FILES)\n",
        "print(\"   Regions:\", ACR_REGIONS)\n",
        "\n",
        "print(\"\\nðŸ“ Rubric folder:\", RUBRIC_DIR)\n",
        "print(\"   Files:\", RUBRIC_FILES)\n",
        "print(\"   Regions:\", RUBRIC_REGIONS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Loading & Input Normalization Helpers"
      ],
      "metadata": {
        "id": "a2jWsiAAVcep"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl8pjCApFRvm"
      },
      "source": [
        "Utility functions that normalize region inputs, safely read PDF/DOCX files, clean extracted text, and load the correct guideline or rubric document for a given anatomical region."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EkVZxG_AFUkA"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# Helpers to stabilize tool I/O\n",
        "# ----------------------------\n",
        "\n",
        "def _normalize_region_key(x: Any) -> str:\n",
        "    \"\"\"\n",
        "    Normalize region_key passed by CrewAI tools.\n",
        "    Handles string, dict, or JSON-string inputs.\n",
        "    \"\"\"\n",
        "    if x is None:\n",
        "        return \"\"\n",
        "\n",
        "    # Plain string or JSON string\n",
        "    if isinstance(x, str):\n",
        "        s = x.strip()\n",
        "        if s.startswith(\"{\") and s.endswith(\"}\"):\n",
        "            try:\n",
        "                obj = json.loads(s)\n",
        "                if isinstance(obj, dict) and \"region_key\" in obj:\n",
        "                    return str(obj[\"region_key\"]).strip().lower()\n",
        "            except Exception:\n",
        "                pass\n",
        "        return s.lower()\n",
        "\n",
        "    # Dict input\n",
        "    if isinstance(x, dict) and \"region_key\" in x:\n",
        "        return str(x[\"region_key\"]).strip().lower()\n",
        "\n",
        "    # Fallback\n",
        "    return str(x).strip().lower()\n",
        "\n",
        "\n",
        "def _clean_text(text: str) -> str:\n",
        "    \"\"\"Normalize whitespace and remove null characters.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = text.replace(\"\\x00\", \" \")\n",
        "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def _read_pdf_text(path: str, max_pages: int = 3, max_chars: int = 12000) -> str:\n",
        "    \"\"\"Read first N pages of a PDF with length cap.\"\"\"\n",
        "    parts = []\n",
        "    with pdfplumber.open(path) as pdf:\n",
        "        for i in range(min(max_pages, len(pdf.pages))):\n",
        "            parts.append(pdf.pages[i].extract_text() or \"\")\n",
        "    txt = _clean_text(\"\\n\\n\".join(parts))\n",
        "    return txt[:max_chars]\n",
        "\n",
        "\n",
        "def _read_docx_text(path: str, max_chars: int = 12000) -> str:\n",
        "    \"\"\"Read DOCX text with length cap.\"\"\"\n",
        "    doc = Document(path)\n",
        "    txt = _clean_text(\"\\n\".join(p.text for p in doc.paragraphs))\n",
        "    return txt[:max_chars]\n",
        "\n",
        "\n",
        "def _load_doc_text(folder: str, region_key: Any, files: List[str]) -> Tuple[bool, str]:\n",
        "    \"\"\"\n",
        "    Load the first matching document for a region.\n",
        "    Returns (success, text_or_error).\n",
        "    \"\"\"\n",
        "    key = _normalize_region_key(region_key)\n",
        "\n",
        "    if not key:\n",
        "        return False, \"Empty region_key.\"\n",
        "\n",
        "    # Match file by extracted region key\n",
        "    candidates = [f for f in files if extract_region_from_filename(f) == key]\n",
        "    if not candidates:\n",
        "        return False, f\"No document found for region '{key}'.\"\n",
        "\n",
        "    path = os.path.join(folder, candidates[0])\n",
        "\n",
        "    try:\n",
        "        if path.lower().endswith(\".pdf\"):\n",
        "            text = _read_pdf_text(path)\n",
        "        elif path.lower().endswith(\".docx\"):\n",
        "            text = _read_docx_text(path)\n",
        "        else:\n",
        "            return False, f\"Unsupported file type: {path}\"\n",
        "    except Exception as e:\n",
        "        return False, f\"Error reading {path}: {e}\"\n",
        "\n",
        "    if not text:\n",
        "        return False, f\"Could not extract text from {path}.\"\n",
        "\n",
        "    return True, text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CrewAI Tools for Standard Documents Retrieval"
      ],
      "metadata": {
        "id": "8o_qLFulWcLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CrewAI tool wrappers that load ACR guidelines or customer rubrics for a given region and return the text or a clear â€œnot foundâ€ message for controlled agent usage."
      ],
      "metadata": {
        "id": "XP6yODmjWiMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def load_acr_guidelines(region_key: str) -> str:\n",
        "    \"\"\"CrewAI tool to load ACR guideline text for a region.\"\"\"\n",
        "    ok, text_or_error = _load_doc_text(ACR_DIR, region_key, ACR_FILES)\n",
        "    return text_or_error if ok else f\"NO_ACR_FOUND: {text_or_error}\"\n",
        "\n",
        "\n",
        "@tool\n",
        "def load_rubric(region_key: str) -> str:\n",
        "    \"\"\"CrewAI tool to load customer rubric text for a region.\"\"\"\n",
        "    ok, text_or_error = _load_doc_text(RUBRIC_DIR, region_key, RUBRIC_FILES)\n",
        "    return text_or_error if ok else f\"NO_RUBRIC_FOUND: {text_or_error}\""
      ],
      "metadata": {
        "id": "9QnzFROzB3F7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent 1: Anatomical Region Classifier"
      ],
      "metadata": {
        "id": "dZLXRJb2be8i"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDbLSfuAFbR7"
      },
      "source": [
        "A strict CrewAI classifier that analyzes the radiology report and returns exactly one valid anatomical region key, enforcing format and allowlist constraints to prevent ambiguous or invalid outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XUs23qKlFeW_"
      },
      "outputs": [],
      "source": [
        "def infer_anatomical_structure(report_text: str, llm) -> str:\n",
        "    \"\"\"\n",
        "    Agent 1 (Classifier):\n",
        "    - Input: raw radiology report text\n",
        "    - Output: ONE lowercase region key (plain text), e.g. \"chest\"\n",
        "\n",
        "    Strict mode:\n",
        "    - No keyword safety-net extraction\n",
        "    - No default fallback\n",
        "    - If output is not exactly one allowed region key, raise ValueError\n",
        "    \"\"\"\n",
        "\n",
        "    # Minimal agent whose only job is to output a region key\n",
        "    classifier_agent = Agent(\n",
        "        role=\"Anatomical Structure Classifier\",\n",
        "        goal=\"Identify the single primary anatomical region for a radiology report.\",\n",
        "        backstory=(\n",
        "            \"You map radiology reports to broad region keys used to select guideline docs. \"\n",
        "            \"Return ONLY the region key.\"\n",
        "        ),\n",
        "        tools=[],            # no tools needed for classification\n",
        "        llm=llm,             # shared FloTorch LLM instance\n",
        "        verbose=False,       # suppress extra agent logs\n",
        "    )\n",
        "\n",
        "    # Single task: strict one-word output contract\n",
        "    task = Task(\n",
        "        description=(\n",
        "            \"You are given a radiology report below.\\n\\n\"\n",
        "            f\"{report_text}\\n\\n\"\n",
        "            \"Return ONLY ONE lowercase anatomical region key.\\n\"\n",
        "            \"Rules:\\n\"\n",
        "            \"- One word only\\n\"\n",
        "            \"- Lowercase\\n\"\n",
        "            \"- No punctuation\\n\"\n",
        "            \"- No explanation\\n\"\n",
        "            \"- Must be one of: chest, abdomen, pelvis, brain, spine, cardiac, renal, liver, echo\\n\"\n",
        "        ),\n",
        "        agent=classifier_agent,\n",
        "        expected_output=\"A single lowercase word like 'chest'\",\n",
        "    )\n",
        "\n",
        "    # Run a tiny crew with just this agent + task\n",
        "    crew = Crew(\n",
        "        # ... keep your existing crew params here (process, memory, etc.)\n",
        "        tasks=[task],\n",
        "        verbose=False,       # suppress crew traces\n",
        "    )\n",
        "\n",
        "    # Execute and normalize\n",
        "    raw = str(crew.kickoff()).strip().lower()\n",
        "\n",
        "    # Enforce strict \"one token, allowlisted\" output\n",
        "    allowed = {\n",
        "        \"chest\", \"abdomen\", \"pelvis\", \"brain\",\n",
        "        \"spine\", \"cardiac\", \"renal\", \"liver\", \"echo\",\n",
        "    }\n",
        "\n",
        "    # Must be exactly one word token (no spaces/newlines)\n",
        "    token = raw.split()\n",
        "    if len(token) != 1:\n",
        "        raise ValueError(\n",
        "            f\"Classifier returned invalid format (expected 1 word). Got: {raw!r}\"\n",
        "        )\n",
        "\n",
        "    region = token[0]\n",
        "\n",
        "    # Must be in allowlist\n",
        "    if region not in allowed:\n",
        "        raise ValueError(\n",
        "            f\"Classifier returned unknown region (not in allowlist). Got: {region!r}\"\n",
        "        )\n",
        "\n",
        "    return region\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anatomical Region Resolution & Normalization"
      ],
      "metadata": {
        "id": "1M83XnBIcm1h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXnrZF3RFgJS"
      },
      "source": [
        "Post-processes the classifier output to reliably resolve a valid anatomical region key using normalization, keyword matching, and report-text heuristics when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1BjRIXFzFi7s"
      },
      "outputs": [],
      "source": [
        "def extract_anatomical_structure_from_output(raw_output: str, report_text: str) -> str:\n",
        "    \"\"\"\n",
        "    Parse and normalize classifier output into a valid region_key.\n",
        "    Falls back to report-text heuristics if needed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Allowed canonical region keys\n",
        "    allowed = {\n",
        "        \"chest\", \"abdomen\", \"pelvis\", \"brain\", \"spine\",\n",
        "        \"renal\", \"cardiac\", \"echo\", \"neck\", \"head\",\n",
        "        \"extremity\", \"msk\",\n",
        "    }\n",
        "\n",
        "    # Normalize raw classifier output\n",
        "    raw = (raw_output or \"\").strip().lower()\n",
        "\n",
        "    # Remove markdown/code fences if present\n",
        "    raw = raw.replace(\"```\", \" \").strip()\n",
        "\n",
        "    # Remove punctuation and split into tokens\n",
        "    raw_clean = re.sub(r\"[^a-z_\\s]\", \" \", raw)\n",
        "    tokens = [t for t in raw_clean.split() if t]\n",
        "\n",
        "    # 1) Return first token that matches an allowed region\n",
        "    for t in tokens:\n",
        "        if t in allowed:\n",
        "            return t\n",
        "\n",
        "    # 2) If output is a sentence, search for allowed keywords\n",
        "    for k in allowed:\n",
        "        if k in raw:\n",
        "            return k\n",
        "\n",
        "    # 3) Heuristic fallback: scan report text for common synonyms\n",
        "    text = (report_text or \"\").lower()\n",
        "    synonyms = [\n",
        "        (\"chest\", \"chest\"),\n",
        "        (\"thorax\", \"chest\"),\n",
        "        (\"lungs\", \"chest\"),\n",
        "        (\"abdomen\", \"abdomen\"),\n",
        "        (\"abdominal\", \"abdomen\"),\n",
        "        (\"pelvis\", \"pelvis\"),\n",
        "        (\"brain\", \"brain\"),\n",
        "        (\"head\", \"head\"),\n",
        "        (\"spine\", \"spine\"),\n",
        "        (\"renal\", \"renal\"),\n",
        "        (\"kidney\", \"renal\"),\n",
        "        (\"cardiac\", \"cardiac\"),\n",
        "        (\"heart\", \"cardiac\"),\n",
        "        (\"echo\", \"echo\"),\n",
        "    ]\n",
        "    for needle, canonical in synonyms:\n",
        "        if needle in text:\n",
        "            return canonical\n",
        "\n",
        "    # Final fallback if nothing matches\n",
        "    return \"chest\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent 2: Standards-Based Radiology Report Validation"
      ],
      "metadata": {
        "id": "-2ETygDKcy4S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKN9vBrJFtYL"
      },
      "source": [
        "Creates the validator agent + task + crew that loads ACR and rubric docs (only when both exist) and outputs a strict, non-leaking validation report with weighted scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fYt1jJJcFn1l"
      },
      "outputs": [],
      "source": [
        "def create_radiology_validator_crew(llm):\n",
        "    # Agent responsible for validating report structure/quality against standards\n",
        "    validator_agent = Agent(\n",
        "        role=\"Radiology Report Validator\",\n",
        "        goal=(\n",
        "            \"Evaluate radiology report documentation quality using standards. \"\n",
        "            \"Use guideline documents ONLY as internal reference.\"\n",
        "        ),\n",
        "        backstory=(\n",
        "            \"You are a senior radiologist assessing documentation quality only. \"\n",
        "            \"Never quote or expose guideline/rubric text. \"\n",
        "            \"Score only when both standards exist.\"\n",
        "        ),\n",
        "        tools=[load_acr_guidelines, load_rubric],  # tools to load ACR + rubric text\n",
        "        llm=llm,                                   # shared FloTorch LLM\n",
        "        verbose=False,                             # keep logs quiet\n",
        "    )\n",
        "\n",
        "    # Task prompt enforces:\n",
        "    # - when to call tools\n",
        "    # - when scoring is allowed\n",
        "    # - strict output template (no markdown/traces)\n",
        "    validate_task = Task(\n",
        "        description=(\n",
        "            \"You are validating a radiology report.\\n\\n\"\n",
        "            \"Inputs:\\n\"\n",
        "            \"- region_key: {region_key}\\n\"\n",
        "            \"- report_text: {report_text}\\n\"\n",
        "            \"- acr_available: {acr_available}\\n\"\n",
        "            \"- rubric_available: {rubric_available}\\n\"\n",
        "            \"- scoring_weights: {scoring_weights}\\n\"\n",
        "            \"- weight_acr: {weight_acr}\\n\"\n",
        "            \"- weight_rubric: {weight_rubric}\\n\\n\"\n",
        "\n",
        "            \"STRICT RULES:\\n\"\n",
        "            \"1) Tool outputs are INTERNAL CONTEXT ONLY.\\n\"\n",
        "            \"2) NEVER paste, quote, or summarize guideline/rubric text.\\n\"\n",
        "            \"3) NEVER output URLs or citations from the guideline docs.\\n\"\n",
        "            \"4) DO NOT include 'Thought:', 'Action:', 'Observation:' lines.\\n\"\n",
        "            \"5) DO NOT use markdown or code fences (NO ```).\\n\"\n",
        "            \"6) Output MUST be ONLY the report text following the template below.\\n\\n\"\n",
        "\n",
        "            \"DECISION LOGIC:\\n\"\n",
        "            \"A) If acr_available == true AND rubric_available == true:\\n\"\n",
        "            \"   - Call load_acr_guidelines(region_key) and load_rubric(region_key)\\n\"\n",
        "            \"   - Validate internally against BOTH\\n\"\n",
        "            \"   - Compute acr_score, rubric_score, overall_score (weighted)\\n\"\n",
        "            \"   - Output numeric scores\\n\\n\"\n",
        "            \"B) Otherwise:\\n\"\n",
        "            \"   - DO NOT call tools\\n\"\n",
        "            \"   - Do NOT judge quality\\n\"\n",
        "            \"   - Overall Adherence Score must be Not Applicable\\n\\n\"\n",
        "\n",
        "            \"OUTPUT TEMPLATE (EXACT â€” DO NOT ADD EXTRA SECTIONS):\\n\"\n",
        "            \"=== Radiology Validation Report ===\\n\"\n",
        "            \"Region: {region_key}\\n\"\n",
        "            \"ACR Used: <Yes/No>\\n\"\n",
        "            \"Rubric Used: <Yes/No>\\n\"\n",
        "            \"Weights: ACR=<weight_acr>, Rubric=<weight_rubric>\\n\"\n",
        "            \"Overall Adherence Score: <XX>/100 OR Not Applicable\\n\"\n",
        "            \"ACR Score: <XX>/100 OR N/A\\n\"\n",
        "            \"Rubric Score: <XX>/100 OR N/A\\n\"\n",
        "            \"\\n\"\n",
        "            \"Documentation Quality Assessment:\\n\"\n",
        "            \"- <bullet>\\n\"\n",
        "            \"- <bullet>\\n\"\n",
        "            \"- <bullet>\\n\"\n",
        "            \"\\n\"\n",
        "            \"Areas for Improvement:\\n\"\n",
        "            \"- <bullet>\\n\"\n",
        "            \"- <bullet>\\n\"\n",
        "            \"\\n\"\n",
        "            \"Notes:\\n\"\n",
        "            \"- <optional bullet>\\n\"\n",
        "        ),\n",
        "        agent=validator_agent,  # runs this task\n",
        "        expected_output=\"Plain-text report only, exactly matching the template.\",\n",
        "    )\n",
        "\n",
        "    # Sequential crew with one agent/task (easy to extend later)\n",
        "    return Crew(\n",
        "        agents=[validator_agent],\n",
        "        tasks=[validate_task],\n",
        "        process=Process.sequential,\n",
        "        verbose=False,  # keep crew output quiet\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validator Output Enforcement & Consistency Guardrails"
      ],
      "metadata": {
        "id": "ExJh3cOgdWOi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlLBgzgoFxwl"
      },
      "source": [
        "Applies deterministic checks to clean the validator output, enforce the required report template, and ensure scoring is consistent with document availability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ubivVCTyFqve"
      },
      "outputs": [],
      "source": [
        "def enforce_validator_output(raw_output: str, acr_available: bool, rubric_available: bool) -> str:\n",
        "    \"\"\"\n",
        "    Enforce stable plain-text output:\n",
        "    - remove ``` fences + Thought/Action/Observation lines\n",
        "    - ensure ONLY ONE Overall Adherence Score line (never both NA + numeric)\n",
        "    - allow numeric scoring only if docs exist + tools ok + output matches template\n",
        "    \"\"\"\n",
        "\n",
        "    txt = (raw_output or \"\").strip()\n",
        "\n",
        "    # 1) Remove markdown fences\n",
        "    txt = txt.replace(\"```\", \"\").strip()\n",
        "\n",
        "    # 2) Remove leaked trace lines (don't truncate whole output)\n",
        "    kept = []\n",
        "    for line in txt.splitlines():\n",
        "        s = line.strip().lower()\n",
        "        if s.startswith(\"thought:\") or s.startswith(\"action:\") or s.startswith(\"observation:\"):\n",
        "            continue\n",
        "        kept.append(line)\n",
        "    txt = \"\\n\".join(kept).strip()\n",
        "\n",
        "    # Detect tool-load failure hints\n",
        "    tool_failed_markers = [\n",
        "        \"not successfully loaded\",\n",
        "        \"could not be loaded\",\n",
        "        \"failed to load\",\n",
        "        \"no_acr_found\",\n",
        "        \"no_rubric_found\",\n",
        "        \"could not extract text\",\n",
        "        \"error reading\",\n",
        "    ]\n",
        "    tool_failed = any(m in txt.lower() for m in tool_failed_markers)\n",
        "\n",
        "    # Template sanity check (prevents scoring on junk outputs)\n",
        "    required_markers = [\n",
        "        \"=== radiology validation report ===\",\n",
        "        \"region:\",\n",
        "        \"acr used:\",\n",
        "        \"rubric used:\",\n",
        "        \"weights:\",\n",
        "        \"documentation quality assessment:\",\n",
        "        \"areas for improvement:\",\n",
        "        \"notes:\",\n",
        "    ]\n",
        "    looks_like_template = all(m in txt.lower() for m in required_markers)\n",
        "\n",
        "    # Scoring allowed only if docs exist + tools ok + template ok\n",
        "    allow_scoring = bool(acr_available and rubric_available and not tool_failed and looks_like_template)\n",
        "\n",
        "    # Matchers for overall score lines\n",
        "    score_line_num = re.compile(\n",
        "        r\"^\\s*Overall Adherence Score:\\s*(\\d{1,3})\\s*/\\s*100\\s*$\",\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "    score_line_na = re.compile(\n",
        "        r\"^\\s*Overall Adherence Score:\\s*Not Applicable\\s*$\",\n",
        "        re.IGNORECASE,\n",
        "    )\n",
        "\n",
        "    lines = txt.splitlines()\n",
        "\n",
        "    # Extract numeric overall score if present and valid\n",
        "    numeric_score = None\n",
        "    for line in lines:\n",
        "        m = score_line_num.match(line)\n",
        "        if m:\n",
        "            val = int(m.group(1))\n",
        "            if 0 <= val <= 100:\n",
        "                numeric_score = val\n",
        "            break\n",
        "\n",
        "    # If scoring is allowed but numeric score missing => force Not Applicable\n",
        "    if allow_scoring and numeric_score is None:\n",
        "        allow_scoring = False\n",
        "\n",
        "    # Remove ALL existing \"Overall Adherence Score\" lines (NA or numeric)\n",
        "    body = [ln for ln in lines if not (score_line_num.match(ln) or score_line_na.match(ln))]\n",
        "\n",
        "    # Choose exactly ONE overall line\n",
        "    if allow_scoring:\n",
        "        overall_line = f\"Overall Adherence Score: {numeric_score}/100\"\n",
        "        # If model had ACR/Rubric as N/A, keep as-is (no contradiction)\n",
        "    else:\n",
        "        overall_line = \"Overall Adherence Score: Not Applicable\"\n",
        "\n",
        "        # Remove numeric ACR/Rubric scores to avoid contradictions\n",
        "        body = [ln for ln in body if not re.match(r\"^\\s*ACR Score:\\s*\\d\", ln, re.IGNORECASE)]\n",
        "        body = [ln for ln in body if not re.match(r\"^\\s*Rubric Score:\\s*\\d\", ln, re.IGNORECASE)]\n",
        "\n",
        "        # Force ACR/Rubric score lines to N/A if present\n",
        "        fixed = []\n",
        "        for ln in body:\n",
        "            if ln.strip().lower().startswith(\"acr score:\"):\n",
        "                fixed.append(\"ACR Score: N/A\")\n",
        "            elif ln.strip().lower().startswith(\"rubric score:\"):\n",
        "                fixed.append(\"Rubric Score: N/A\")\n",
        "            else:\n",
        "                fixed.append(ln)\n",
        "        body = fixed\n",
        "\n",
        "    # Insert overall line right after Weights (preferred)\n",
        "    out = []\n",
        "    inserted = False\n",
        "    for ln in body:\n",
        "        out.append(ln)\n",
        "        if (not inserted) and ln.strip().lower().startswith(\"weights:\"):\n",
        "            out.append(overall_line)\n",
        "            inserted = True\n",
        "\n",
        "    # Fallback placement if Weights line missing\n",
        "    if not inserted:\n",
        "        out.insert(0, overall_line)\n",
        "\n",
        "    return \"\\n\".join(out).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End-to-End Radiology Validation Orchestration"
      ],
      "metadata": {
        "id": "8_N_09aZd0cT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TGIEy1DNHY_"
      },
      "source": [
        "Orchestrates the full validation flowâ€”region classification, document availability checks, CrewAI validator execution, retry on off-template output, and final output enforcement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gxwM3CUdSaEN"
      },
      "outputs": [],
      "source": [
        "def _looks_like_template(txt: str) -> bool:\n",
        "    \"\"\"Basic check that output matches the required template structure.\"\"\"\n",
        "    t = (txt or \"\").lower()\n",
        "    required = [\n",
        "        \"=== radiology validation report ===\",\n",
        "        \"region:\",\n",
        "        \"acr used:\",\n",
        "        \"rubric used:\",\n",
        "        \"weights:\",\n",
        "        \"documentation quality assessment:\",\n",
        "        \"areas for improvement:\",\n",
        "        \"notes:\",\n",
        "    ]\n",
        "    return all(x in t for x in required)\n",
        "\n",
        "\n",
        "def _preflight_load_docs(region_key: str):\n",
        "    \"\"\"\n",
        "    Preflight load docs using the plain Python loader (NOT the @tool wrappers).\n",
        "    Returns: (acr_available, rubric_available)\n",
        "    \"\"\"\n",
        "    acr_ok, acr_text_or_err = _load_doc_text(ACR_DIR, region_key, ACR_FILES)\n",
        "    rub_ok, rub_text_or_err = _load_doc_text(RUBRIC_DIR, region_key, RUBRIC_FILES)\n",
        "    return acr_ok, rub_ok\n",
        "\n",
        "\n",
        "def run_full_radiology_validation(\n",
        "    report_text: str,\n",
        "    scoring_weights: Dict[str, float] = {\"acr\": 0.5, \"rubric\": 0.5},\n",
        "    silence_console: bool = True,\n",
        ") -> str:\n",
        "    # Refresh file lists (in case Drive contents changed)\n",
        "    global ACR_FILES, RUBRIC_FILES\n",
        "    ACR_FILES = _list_docs(ACR_DIR)\n",
        "    RUBRIC_FILES = _list_docs(RUBRIC_DIR)\n",
        "\n",
        "    # 1) Classify region (one-word contract)\n",
        "    raw_classification = infer_anatomical_structure(report_text, model)\n",
        "    region_key = extract_anatomical_structure_from_output(raw_classification, report_text)\n",
        "\n",
        "    # 2) Preflight load docs (real availability, not just filenames)\n",
        "    acr_available, rubric_available = _preflight_load_docs(region_key)\n",
        "\n",
        "    # 3) Normalize weights to sum to 1.0\n",
        "    acr_w = float(scoring_weights.get(\"acr\", 0.5))\n",
        "    rub_w = float(scoring_weights.get(\"rubric\", 0.5))\n",
        "    total = acr_w + rub_w\n",
        "    if total <= 0:\n",
        "        acr_w, rub_w, total = 0.5, 0.5, 1.0\n",
        "    weights = {\"acr\": acr_w / total, \"rubric\": rub_w / total}\n",
        "\n",
        "    # 4) Build validator crew\n",
        "    validator_crew = create_radiology_validator_crew(model)\n",
        "\n",
        "    inputs = {\n",
        "        \"region_key\": region_key,\n",
        "        \"report_text\": report_text,\n",
        "        \"acr_available\": acr_available,\n",
        "        \"rubric_available\": rubric_available,\n",
        "        \"scoring_weights\": weights,\n",
        "        \"weight_acr\": weights[\"acr\"],\n",
        "        \"weight_rubric\": weights[\"rubric\"],\n",
        "    }\n",
        "\n",
        "    def _kickoff():\n",
        "        if silence_console:\n",
        "            buf = io.StringIO()\n",
        "            with contextlib.redirect_stdout(buf), contextlib.redirect_stderr(buf):\n",
        "                return validator_crew.kickoff(inputs=inputs)\n",
        "        return validator_crew.kickoff(inputs=inputs)\n",
        "\n",
        "    # 5) Run + retry once if off-template\n",
        "    result = _kickoff()\n",
        "    raw = str(result)\n",
        "\n",
        "    if not _looks_like_template(raw):\n",
        "        result = _kickoff()\n",
        "        raw = str(result)\n",
        "\n",
        "    # 6) Enforce final output contract\n",
        "    return enforce_validator_output(\n",
        "        raw_output=raw,\n",
        "        acr_available=acr_available,\n",
        "        rubric_available=rubric_available,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAJNeq-B_47q"
      },
      "source": [
        "## Test Cases"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test case 1: Defines a sample chest radiology report, sets scoring weights, runs the full validation pipeline end-to-end, and prints the final structured validation report output."
      ],
      "metadata": {
        "id": "cOb8DiwFeAmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3gNLndJH2yb",
        "outputId": "420815aa-cb95-4c65-da6c-da5b5f1ceab0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running Chest Validation Test (ACR + Rubric present) ===\n",
            "\n",
            "[EventBus Error] Handler 'on_task_started' failed for event 'TaskStartedEvent': 'NoneType' object has no attribute 'key'\n",
            "=== Radiology Validation Report ===\n",
            "Region: chest\n",
            "ACR Used: Yes\n",
            "Rubric Used: Yes\n",
            "Weights: ACR=0.6, Rubric=0.4\n",
            "Overall Adherence Score: 87/100\n",
            "ACR Score: 95/100\n",
            "Rubric Score: 74/100\n",
            "\n",
            "Documentation Quality Assessment:\n",
            "- Appropriate imaging study ordered for specific clinical indication (endotracheal tube placement evaluation), aligning with ACR principles that exclude routine imaging\n",
            "- Comprehensive anatomical coverage with all required chest structures systematically evaluated\n",
            "- Quantitative measurement provided for endotracheal tube position (4.5 cm above carina)\n",
            "- Clear documentation of negative findings including absence of consolidation, pleural effusion, and pneumothorax\n",
            "- Appropriate acknowledgment of portable technique limitations on cardiac silhouette assessment\n",
            "\n",
            "Areas for Improvement:\n",
            "- Add a separate Impression section to summarize key findings and directly answer the clinical question\n",
            "- Include explicit statement regarding presence or absence of pulmonary nodules or masses\n",
            "- Consider adding cardiothoracic ratio measurement or more specific cardiac size description\n",
            "- Provide follow-up recommendations if clinically indicated (e.g., repeat imaging if tube repositioning needed)\n",
            "\n",
            "Notes:\n",
            "- Report demonstrates strong adherence to anatomical documentation standards\n",
            "- Clinical indication appropriately addressed with relevant tube position measurements\n",
            "- Absence of comparison studies appropriately documented\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Test Case: Chest Radiograph (ACR + Rubric available)\n",
        "#\n",
        "# Expected behavior:\n",
        "# - Agent 1 classifies region as \"chest\"\n",
        "# - System finds BOTH:\n",
        "#     âœ” ACR chest guideline\n",
        "#     âœ” Customer chest rubric\n",
        "# - Validator agent:\n",
        "#     âœ” Calls both tools (ACR + rubric)\n",
        "#     âœ” Compares report against both\n",
        "#     âœ” Returns detailed feedback\n",
        "#     âœ” Returns an adherence score\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Sample chest radiology report\n",
        "test_query = (\n",
        "    \"FINAL REPORT. \"\n",
        "    \"EXAMINATION: CHEST (PORTABLE AP). \"\n",
        "    \"INDICATION: Evaluation of endotracheal tube placement. \"\n",
        "    \"TECHNIQUE: Portable AP radiograph of the chest. \"\n",
        "    \"COMPARISON: None. \"\n",
        "    \"FINDINGS: Endotracheal tube tip terminates approximately 4.5 cm above the carina. \"\n",
        "    \"Enteric tube courses below the diaphragm with tip projecting over the stomach. \"\n",
        "    \"Cardiomediastinal silhouette is within normal limits for portable technique. \"\n",
        "    \"Lungs are clear without focal consolidation, pleural effusion, or pneumothorax. \"\n",
        "    \"No acute osseous abnormality identified.\"\n",
        ")\n",
        "\n",
        "print(\"=== Running Chest Validation Test (ACR + Rubric present) ===\\n\")\n",
        "\n",
        "# Optional: customize scoring weights (user-controlled)\n",
        "scoring_weights = {\n",
        "    \"acr\": 0.6,      # 60% weight to ACR compliance\n",
        "    \"rubric\": 0.4,   # 40% weight to customer rubric\n",
        "}\n",
        "\n",
        "# Run the full validation pipeline\n",
        "result = run_full_radiology_validation(\n",
        "    report_text=test_query,\n",
        "    scoring_weights=scoring_weights,\n",
        ")\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Test case 2: Defines a sample abdominal ultrasound (gallbladder) report, sets scoring weights, runs the full validation pipeline end-to-end, and prints the final structured validation report output."
      ],
      "metadata": {
        "id": "Eagmz_lQej2Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rFN-i8TRczq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc3f668a-2498-453c-af38-d4af9f1815ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running Abdominal Ultrasound (Gallbladder) Validation Test ===\n",
            "\n",
            "[EventBus Error] Handler 'on_task_started' failed for event 'TaskStartedEvent': 'NoneType' object has no attribute 'key'\n",
            "\n",
            "=== Final Output ===\n",
            "=== Radiology Validation Report ===\n",
            "Region: abdomen\n",
            "ACR Used: No\n",
            "Rubric Used: No\n",
            "Weights: ACR=0.6, Rubric=0.4\n",
            "Overall Adherence Score: Not Applicable\n",
            "ACR Score: N/A\n",
            "Rubric Score: N/A\n",
            "\n",
            "Documentation Quality Assessment:\n",
            "- No reference standards available for validation\n",
            "- Report contains anatomical findings for gallbladder, biliary system, solid organs, and kidneys\n",
            "- Measurements provided for kidneys and common hepatic duct\n",
            "\n",
            "Areas for Improvement:\n",
            "- Cannot assess without reference standards\n",
            "- Validation requires either ACR guidelines or institutional rubric to be available\n",
            "\n",
            "Notes:\n",
            "- This report cannot be scored without access to validation standards\n",
            "- To enable quality assessment, please provide either ACR guidelines or institutional rubric for the abdomen region\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------\n",
        "# Test Case: Abdominal Ultrasound â€“ Gallbladder (ACR + Rubric if available)\n",
        "#\n",
        "# Expected behavior:\n",
        "# - Agent 1 classifies region as \"abdomen\" (or \"gallbladder\" â†’ abdomen)\n",
        "# - System checks:\n",
        "#     âœ” ACR abdominal ultrasound guideline (if present)\n",
        "#     âœ” Customer abdominal / gallbladder rubric (if present)\n",
        "# - Validator agent:\n",
        "#     âœ” Calls available tools (ACR + rubric only if BOTH exist)\n",
        "#     âœ” Compares report against available standards\n",
        "#     âœ” Returns detailed feedback\n",
        "#     âœ” Returns an adherence score ONLY if both ACR + rubric exist\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "# Sample abdominal ultrasound (gallbladder) radiology report\n",
        "test_query = (\n",
        "    \"FINDINGS: THE GALLBLADDER IS WELL VISUALIZED AND NO INTRALUMINAL STONE, \"\n",
        "    \"WALL THICKENING OR PERICHOLECYSTIC FLUID SEEN. COMMON HEPATIC DUCT IS NORMAL \"\n",
        "    \"IN CALIBER AT 3MM. AORTA, IVC, AND IMAGED SEGMENTS OF THE PANCREAS ARE NORMAL. \"\n",
        "    \"THE LIVER AND SPLEEN ARE NORMAL. THE RIGHT KIDNEY MEASURES 12.7 CM IN LENGTH. \"\n",
        "    \"THE LEFT KIDNEY MEASURES 12.6 CM. THERE IS NO HYDRONEPHROSIS OR RENAL MASS.\"\n",
        ")\n",
        "\n",
        "print(\"=== Running Abdominal Ultrasound (Gallbladder) Validation Test ===\\n\")\n",
        "\n",
        "# Optional: customize scoring weights (user-controlled)\n",
        "scoring_weights = {\n",
        "    \"acr\": 0.6,      # 60% weight to ACR compliance\n",
        "    \"rubric\": 0.4,   # 40% weight to customer rubric\n",
        "}\n",
        "\n",
        "# Run the full validation pipeline\n",
        "result = run_full_radiology_validation(\n",
        "    report_text=test_query,\n",
        "    scoring_weights=scoring_weights,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Final Output ===\")\n",
        "print(result)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}